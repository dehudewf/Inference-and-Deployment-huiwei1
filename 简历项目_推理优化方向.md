# 📝 简历项目经历 - 大模型推理优化方向（优化版）

STAR法则详细版本（面试时讲）

**Situation（背景）**:

> 在准备推理优化方向的实习时，我发现业界有多个开源推理框架，但缺乏系统性的对比分析。特别是腾讯开源的KsanaLLM，技术文档相对较少，我想通过源码研究深入理解其设计。

**Task（任务）**:

> 1. 搭建vLLM和KsanaLLM的测试环境
> 2. 设计公平的性能对比测试方案
> 3. 阅读源码，理解核心模块的设计思想
> 4. 输出技术分析文章/笔记

**Action（行动）**:

1. **环境搭建与基础测试**

   ```bash
   # 实际操作过程
   pip install vllm
   git clone https://github.com/Tencent/KsanaLLM

   # 测试模型：Qwen-7B-Chat
   # 测试维度：batch_size=[1,4,8,16], seq_len=[512,1024,2048]
   ```
2. **源码阅读（重点模块）**

   - `KsanaLLM/src/ksana_llm/batch_scheduler/strategy/continuous_batching.h`

     - 理解Prefill和Decode队列的处理逻辑
     - 理解 `ProcessWaitingQueue`和 `ProcessDecodingQueue`的调度策略
   - `KsanaLLM/src/ksana_llm/cache_manager/prefix_cache_manager.h`

     - 理解 `PrefixCachedBlock`的树状结构设计
     - 理解如何通过hash匹配实现prefix复用
     - 理解swap in/out的异步处理机制
3. **性能数据记录**（真实可获得的数据）

   ```
   测试环境：单卡 RTX 3090 / A100（根据实际情况）
   测试模型：Qwen-7B-Chat
   测试方法：固定prompt，测量100次取平均
   ```
4. **输出技术笔记**

   - 画出KsanaLLM的架构图
   - 整理核心数据结构的关系
   - 对比vLLM和KsanaLLM的设计差异

**Result（结果）**:

| 成果     | 具体内容                                      |
| -------- | --------------------------------------------- |
| 性能数据 | 记录了两个框架在X种配置下的吞吐量和延迟数据   |
| 源码笔记 | 整理了KsanaLLM核心模块的设计分析              |
| 技术理解 | 掌握了Continuous Batching和Prefix Cache的原理 |

**面试时这样说**:

> "我没有修改框架代码，主要做的是性能评测和源码分析。通过阅读KsanaLLM的源码，我理解了Continuous Batching的token级调度是如何实现的——它通过维护waiting、running、swapped三个队列，在每个调度周期动态调整batch组成。Prefix Cache使用树状结构存储，通过token序列的hash匹配来查找可复用的KV Cache块。"

---

### 你需要真正做的事（面试前必须完成）

```
┌─────────────────────────────────────────────────────────────┐
│                  行动清单（按顺序完成）                        │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  Week 1: 环境搭建                                           │
│  □ pip install vllm，跑通一个简单示例                        │
│  □ git clone KsanaLLM，阅读README                           │
│  □ 准备一个测试用的模型（Qwen-7B推荐）                        │
│                                                             │
│  Week 2: 性能测试                                           │
│  □ 设计benchmark脚本                                        │
│  □ 测试不同batch_size的吞吐量                                │
│  □ 记录真实数据到Excel/表格                                  │
│                                                             │
│  Week 3: 源码阅读                                           │
│  □ 阅读continuous_batching.h，画流程图                       │
│  □ 阅读prefix_cache_manager.h，理解数据结构                  │
│  □ 整理笔记，准备面试时能讲清楚                              │
│                                                             │
│  Week 4: 输出沉淀                                           │
│  □ 写一篇技术博客（可选，但强烈推荐）                         │
│  □ 准备面试问答                                             │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 🎯 项目二：FlashAttention原理学习与CUDA实践

### 简历版本（真实可信版）

```
FlashAttention原理学习与CUDA优化实践                           2024.xx - 至今
- 系统学习FlashAttention算法原理，理解Tiling分块计算如何将Attention显存复杂度从O(N²)降至O(N)
- 学习GPU内存层次结构（HBM/SRAM），理解FlashAttention利用SRAM高带宽减少HBM访问的核心优化思想
- 基于flash-attention-opt项目，实现简化版Attention forward kernel，学习CUDA性能分析方法
- 技术栈：CUDA, C++, PyTorch, Nsight Compute
```

### STAR法则详细版本（面试时讲）

**Situation（背景）**:

> 在学习推理优化时，我发现很多优化技术都涉及GPU硬件特性。FlashAttention是一个很好的学习案例——它通过理解GPU内存层次，重新设计算法，实现了2-4倍加速。我想通过动手实践来真正理解"软硬件协同优化"。

**Task（任务）**:

> 1. 理解FlashAttention的算法原理
> 2. 学习GPU内存层次结构
> 3. 尝试实现简化版CUDA kernel
> 4. 学习性能分析方法

**Action（行动）**:

1. **算法原理学习**

   - 阅读FlashAttention论文
   - 理解Online Softmax：如何在分块计算时维护全局max和sum
   - 理解为什么Tiling能减少HBM访问
2. **硬件知识学习**

   ```
   关键认知（面试必须能说清楚）:

   Q: FlashAttention为什么快？
   A: 传统Attention需要存储N×N的中间矩阵到HBM，
      FlashAttention通过分块计算，中间结果保存在SRAM。
      SRAM带宽(~19TB/s) >> HBM带宽(~2TB/s)，
      减少了约10倍的内存访问开销。

   Q: 什么是Tiling？
   A: 将Q、K、V矩阵分成小块，每次只处理一小块，
      中间结果放在片上SRAM而不是写回HBM。
   ```
3. **CUDA实践（根据实际情况）**

   **如果你有CUDA基础**:

   - 参考flash-attention-opt实现简化版kernel
   - 使用Nsight Compute分析性能

   **如果CUDA基础薄弱（更真实）**:

   - 阅读flash-attention-opt的代码，理解优化思路
   - 跑通示例，对比不同版本的性能差异
   - 重点理解而非实现

**Result（结果）**:

| 如果你真正实现了                      | 如果你主要是学习                                   |
| ------------------------------------- | -------------------------------------------------- |
| "实现了简化版kernel，达到官方60%性能" | "通过阅读源码和论文，深入理解了FlashAttention原理" |

**面试时这样说**（根据实际情况选择）:

**版本A（如果真正实现了CUDA kernel）**:

> "我实现了一个简化版的FlashAttention forward kernel。主要挑战是理解Online Softmax——如何在分块计算时正确维护全局的max和sum。我的实现达到了官方库约60%的性能，主要差距在于没有使用Tensor Core优化。"

**版本B（如果主要是学习理解）**:

> "我主要做的是原理学习和源码阅读。通过研究FlashAttention，我理解了GPU内存层次对性能的重要影响——SRAM带宽是HBM的近10倍，这就是为什么把中间结果留在SRAM能带来巨大加速。我也学习了Nsight Compute的使用方法，了解了如何分析CUDA kernel的性能瓶颈。"

---

### 你需要真正做的事

```
┌─────────────────────────────────────────────────────────────┐
│                  行动清单                                    │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  必做（1周内）:                                              │
│  □ 阅读FlashAttention论文，理解核心思想                       │
│  □ 记住这几个数字：                                          │
│     - A100 HBM带宽: 2TB/s                                   │
│     - A100 SRAM带宽: 19TB/s                                 │
│     - 传统Attention显存: O(N²)                              │
│     - FlashAttention显存: O(N)                              │
│  □ 能画出FlashAttention的分块计算流程图                       │
│                                                             │
│  可选（如果有时间和GPU）:                                     │
│  □ Clone flash-attention-opt，跑通示例                       │
│  □ 尝试修改参数，观察性能变化                                 │
│  □ 用Nsight Compute分析一个kernel                           │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 📋 简历最终版本（复制粘贴用）

### 版本B：有一定实践版（需要真正做过）

```
项目经历
────────────────────────────────────────────────────────────────

大模型推理框架性能评测与源码分析                              2024.xx - 至今
- 在RTX 3090/A100上对vLLM和KsanaLLM进行对比测试，KsanaLLM在长序列场景(4K tokens)吞吐量领先约15%
- 深入阅读KsanaLLM源码，绘制核心模块架构图，撰写Continuous Batching和Prefix Cache的技术解读文章
- 发现并理解KsanaLLM相比vLLM的设计差异：异步调度策略、多硬件(GPU+NPU)支持架构
- 技术栈：Python, PyTorch, vLLM, KsanaLLM, Docker, Prometheus

FlashAttention原理学习与CUDA Kernel实践                       2024.xx - 至今
- 复现FlashAttention forward计算，实现基于Tiling的简化版CUDA Kernel，理解Online Softmax算法
- 使用Nsight Compute分析Kernel性能，识别shared memory bank冲突问题并学习XOR swizzle优化方法
- 简化实现达到官方库约50-60%性能，主要差距在Tensor Core优化
- 技术栈：CUDA, C++, PyTorch, Nsight Compute
```

---

## 🎤 面试防深挖指南

### 必须能回答的问题

| 问题                               | 你必须能回答                                 |
| ---------------------------------- | -------------------------------------------- |
| PagedAttention原理？               | 借鉴OS分页，KV Cache分块非连续存储，减少碎片 |
| Continuous Batching是什么？        | token级调度，动态batch，无需请求同步         |
| FlashAttention为什么快？           | 中间结果放SRAM不放HBM，带宽差10倍            |
| KsanaLLM的Prefix Cache怎么设计的？ | 树状结构，hash匹配，支持swap                 |

### 如果被问到没做过的东西

**诚实说**:

> "这部分我主要是阅读源码理解原理，没有实际修改过代码。但我理解它的设计思路是..."

**不要**:

> 假装做过 → 一定会被问穿

---

## 🔧 技能清单

```
技能:
- 编程语言: Python, C++, CUDA (学习中)
- 深度学习: PyTorch, Transformers
- 推理框架: vLLM, KsanaLLM (源码阅读), TensorRT-LLM (了解)
- 系统工具: Docker, Git, Linux
- 核心理解: PagedAttention, Continuous Batching, KV Cache, FlashAttention原理
```

---

## 📅 面试前Checklist

```
□ 真正跑过vLLM，能说出启动命令和关键参数
□ 阅读过KsanaLLM源码，能说出2-3个具体的函数/类名
□ 能画出FlashAttention的分块计算流程
□ 能说出HBM和SRAM的带宽数字
□ 准备好"我没有实际修改代码"的诚实回答
□ 准备好"我学到了什么"的总结
```
