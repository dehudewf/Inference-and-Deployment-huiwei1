/* Copyright 2025 Tencent Inc.  All rights reserved.

==============================================================================*/
#include "ksana_llm/cache_manager/block_allocator/block_allocator.h"

#include "fmt/core.h"
#include "ksana_llm/utils/context.h"
#include "ksana_llm/utils/device_types.h"
#include "ksana_llm/utils/device_utils.h"
#include "ksana_llm/utils/dynamic_memory_pool.h"
#include "ksana_llm/utils/logger.h"
#include "ksana_llm/utils/string_utils.h"

namespace ksana_llm {

BlockAllocator::BlockAllocator(MemoryLocation location, size_t block_num, size_t block_size, int rank,
                               std::shared_ptr<MemoryAllocatorInterface> memory_allocator,
                               std::shared_ptr<Context> context)
    : location_(location),
      block_num_(block_num),
      block_size_(block_size),
      rank_(rank),
      memory_allocator_(memory_allocator),
      context_(context) {
  using namespace std::placeholders;
  if (location_ == MemoryLocation::LOCATION_HOST) {
    malloc_fn_ = std::bind(&MemoryAllocatorInterface::HostAlloc, memory_allocator.get(), _1, _2);
    free_fn_ = std::bind(&MemoryAllocatorInterface::HostFree, memory_allocator.get(), _1);
  } else if (location_ == MemoryLocation::LOCATION_DEVICE) {
    malloc_fn_ = std::bind(&MemoryAllocatorInterface::MallocAsync, memory_allocator.get(), _1, _2,
                           context_->GetMemoryManageStreams()[rank_]);
    free_fn_ = std::bind(&MemoryAllocatorInterface::FreeAsync, memory_allocator.get(), _1,
                         context_->GetMemoryManageStreams()[rank_]);
  } else {
    KLLM_THROW("The MemoryLocation is not supported.");
  }
}

BlockAllocator::~BlockAllocator() { Clear(); }

void BlockAllocator::PreAllocateBlocks() {
  bool use_continuous_memory = false;
  void* base_mem_ptr = nullptr;

  if (location_ == MemoryLocation::LOCATION_DEVICE) {
    SetDevice(rank_);
  }

#if defined(ENABLE_ACL) || defined(ENABLE_FLASH_ATTN_WITH_CACHE)
  if (location_ == MemoryLocation::LOCATION_DEVICE) {
    use_continuous_memory = true;
    size_t dev_bytes = (block_num_ + 1) * block_size_;
    if (!DeviceMemoryPool::Empty()) {
      base_mem_ptr = DeviceMemoryPool::GetMemoryPool(rank_)->Allocate(dev_bytes, true);
    } else {
      malloc_fn_(reinterpret_cast<void**>(&base_mem_ptr), dev_bytes);
    }
    blocks_base_ptr_ = base_mem_ptr;
  }
#endif

  // For host memory, always use continuous allocation to avoid repeated cudaHostAlloc calls
  if (location_ == MemoryLocation::LOCATION_HOST && block_num_ > 0) {
    use_continuous_memory = true;
    size_t host_bytes = block_num_ * block_size_;
    malloc_fn_(reinterpret_cast<void**>(&base_mem_ptr), host_bytes);
    blocks_base_ptr_ = base_mem_ptr;
  }

  // NOTE: Make sure block ids on all worker nodes have same id range.
  free_blocks_.reserve(block_num_);
  void* memory_ptr = nullptr;
  for (size_t block_id = 0; block_id < block_num_; ++block_id) {
    if (use_continuous_memory) {
      memory_ptr = reinterpret_cast<uint8_t*>(base_mem_ptr) + block_id * block_size_;
    } else {
      if (location_ == MemoryLocation::LOCATION_DEVICE && !DeviceMemoryPool::Empty()) {
        memory_ptr = DeviceMemoryPool::GetMemoryPool(rank_)->Allocate(block_size_, true);
      } else {
        malloc_fn_(&memory_ptr, block_size_);
      }
    }
    free_blocks_.emplace(block_id, memory_ptr);
  }
}

void BlockAllocator::Clear() {
  if (location_ == MemoryLocation::LOCATION_DEVICE) {
    SetDevice(rank_);
  }

#if defined(ENABLE_ACL_ATB) || defined(ENABLE_FLASH_ATTN_WITH_CACHE)
  if (location_ == MemoryLocation::LOCATION_DEVICE) {
    std::unique_lock<std::shared_mutex> lock(shared_mutex_);

    if (blocks_base_ptr_ != nullptr) {
      if (location_ == MemoryLocation::LOCATION_DEVICE && !DeviceMemoryPool::Empty()) {
        DeviceMemoryPool::GetMemoryPool(rank_)->Free(blocks_base_ptr_);
      } else {
        free_fn_(blocks_base_ptr_);
      }
      blocks_base_ptr_ = nullptr;
    }
    free_blocks_.clear();
    used_blocks_.clear();
    return;
  }
#endif

  // For host memory with continuous allocation, just free the base pointer
  if (location_ == MemoryLocation::LOCATION_HOST && blocks_base_ptr_ != nullptr) {
    std::unique_lock<std::shared_mutex> lock(shared_mutex_);
    free_fn_(blocks_base_ptr_);
    blocks_base_ptr_ = nullptr;
    free_blocks_.clear();
    used_blocks_.clear();
    return;
  }

  // For non-continuous allocation (legacy path)
  {
    auto clear_fn = [&](std::unordered_map<int, void*>& blocks) -> void {
      for (auto it = blocks.begin(); it != blocks.end();) {
        if (location_ == MemoryLocation::LOCATION_DEVICE && !DeviceMemoryPool::Empty()) {
          DeviceMemoryPool::GetMemoryPool(rank_)->Free(it->second);
        } else {
          free_fn_(it->second);
        }
        it = blocks.erase(it);
      }
    };

    std::unique_lock<std::shared_mutex> lock(shared_mutex_);
    clear_fn(free_blocks_);
    clear_fn(used_blocks_);
  }
}

Status BlockAllocator::AllocateBlocks(size_t block_num, std::vector<int>& blocks) {
  std::unique_lock<std::shared_mutex> lock(shared_mutex_);

  if (block_num > free_blocks_.size()) {
    return Status(RET_DEVICE_MEM_ALLOCATE_FAILED,
                  FormatStr("No more free blocks, expect %d, free %d", block_num, free_blocks_.size()));
  }

  blocks.clear();
  blocks.reserve(block_num);
  auto it = free_blocks_.begin();
  while (block_num--) {
    used_blocks_.insert(*it);
    blocks.push_back(it->first);
    it = free_blocks_.erase(it);
  }
  return Status();
}

Status BlockAllocator::FreeBlocks(const std::vector<int>& blocks) {
  std::unique_lock<std::shared_mutex> lock(shared_mutex_);

  for (auto block_id : blocks) {
    auto it = used_blocks_.find(block_id);
    if (it != used_blocks_.end()) {
      free_blocks_.insert(*it);
      used_blocks_.erase(it);
    } else {
      return Status(RET_DEVICE_MEM_FREE_FAILED, fmt::format("Double free error, block id {}", block_id));
    }
  }
  return Status();
}

// get block pointer to addrs
Status BlockAllocator::GetBlockPtrs(const std::vector<int>& blocks, std::vector<void*>& addrs) {
  addrs.clear();
  return AppendBlockPtrs(blocks, addrs);
}

// append new block pointers to addrs
Status BlockAllocator::AppendBlockPtrs(const std::vector<int>& blocks, std::vector<void*>& addrs) {
  const size_t exist_blocks = addrs.size();
  if (exist_blocks == blocks.size()) {
    return Status();
  }

  addrs.resize(blocks.size());
  std::shared_lock<std::shared_mutex> lock(shared_mutex_);
  for (size_t i = exist_blocks; i < blocks.size(); ++i) {
    const int block_id = blocks[i];
    if (const auto it = used_blocks_.find(block_id); it != used_blocks_.end()) {
      addrs[i] = it->second;
      continue;
    }

    // For distributed worker node, get from free blocks.
    if (!context_->IsChief()) {
      if (const auto it = free_blocks_.find(block_id); it != free_blocks_.end()) {
        addrs[i] = it->second;
        continue;
      }
    }

    KLLM_LOG_ERROR << "Get block id " << block_id << " address error on device " << rank_;
    return Status(RET_SEGMENT_FAULT, FormatStr("Get block address error, block id {}", block_id));
  }
  return Status();
}

void* BlockAllocator::GetBlocksBasePtr() { return blocks_base_ptr_; }

int BlockAllocator::GetBlocksBaseId() { return blocks_base_id_; }

size_t BlockAllocator::GetFreeBlockNumber() {
  std::shared_lock<std::shared_mutex> lock(shared_mutex_);
  return free_blocks_.size();
}

size_t BlockAllocator::GetUsedBlockNumber() {
  std::shared_lock<std::shared_mutex> lock(shared_mutex_);
  return used_blocks_.size();
}

}  // namespace ksana_llm
