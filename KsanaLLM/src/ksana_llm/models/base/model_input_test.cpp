/* Copyright 2024 Tencent Inc.  All rights reserved.

==============================================================================*/

#include <pybind11/pybind11.h>
#include <torch/csrc/autograd/python_variable.h>
#include <torch/torch.h>

#include <cstring>
#include <filesystem>
#include <random>

#ifdef ENABLE_CUDA
#  include "csrc/kernels/nvidia/flash_mla/flash_mla.h"
#endif
#include "ksana_llm/models/base/model_input.h"
#include "ksana_llm/utils/singleton.h"
#include "tests/test.h"

namespace py = pybind11;

namespace ksana_llm {

class ModelInputTest : public testing::Test {
 protected:
  void SetUp() override {
    int rank = 0;
    auto context = std::make_shared<Context>(1, 1, 1);

    // Parse the yaml config file.
    const auto& env = Singleton<Environment>::GetInstance();
    std::filesystem::path current_path = __FILE__;
    std::filesystem::path parent_path = current_path.parent_path();
    std::filesystem::path config_path_relate = parent_path / "../../../../examples/llama7b/ksana_llm.yaml";
    std::string config_path = std::filesystem::absolute(config_path_relate).string();
    env->ParseConfig(config_path);

    // Initialize the model config
    ModelConfig model_config;
    env->GetModelConfig(model_config);

    // 修改kv_lora_rank为512
    model_config.mla_config.kv_lora_rank = 512;

    // Initialize the block manager.
    env->InitializeBlockManagerConfig();
    BlockManagerConfig block_manager_config;
    env->GetBlockManagerConfig(block_manager_config);
    block_manager_config.block_host_memory_factor = 0.0;
    block_manager_config.device_allocator_config.blocks_num = 10;  // This test just need a few blocks;
    block_manager_config.host_allocator_config.blocks_num = block_manager_config.device_allocator_config.blocks_num;
    env->SetBlockManagerConfig(block_manager_config);

    RuntimeConfig runtime_config;
    env->GetRuntimeConfig(runtime_config);

    const auto* test_info = ::testing::UnitTest::GetInstance()->current_test_info();
    std::string test_name = test_info->name();
    if (test_name.find("PrepareFlashRotaryMlaFlexibleCacheTest") != std::string::npos) {
      runtime_config.attn_backend_config.enable_blocked_multi_token_forwarding_kv = true;
      runtime_config.enable_flexible_caching = true;
      runtime_config.mtp_step_num = 0;
    }

    // Initialize the model input object.
    model_config.use_mla = true;
    model_input = std::make_unique<ModelInput>(model_config, runtime_config, rank, context);

    // Initialize the random seed with 0.
    std::srand(0);
  }

  void TearDown() override {}

 protected:
  std::unique_ptr<ModelInput> model_input;
};

TEST_F(ModelInputTest, PrepareInputRefitTest) {
  std::vector<float*> input_refit_emb_ptr;
  std::vector<std::pair<int64_t, int64_t>> input_refit_pos_pair;

  auto VerifyPrepareInputRefit = [&]() {
    const size_t input_refit_size = input_refit_emb_ptr.size();
    EXPECT_EQ(model_input->cpu_input_refit_tensor.emb_fp32_ptr_tensor.shape.size(), 1);
    EXPECT_EQ(model_input->cpu_input_refit_tensor.emb_fp32_ptr_tensor.shape[0], input_refit_size);
    EXPECT_EQ(model_input->cpu_input_refit_tensor.pos_pair_tensor.shape.size(), 2);
    EXPECT_EQ(model_input->cpu_input_refit_tensor.pos_pair_tensor.shape[0], input_refit_size);
    EXPECT_EQ(model_input->cpu_input_refit_tensor.pos_pair_tensor.shape[1], 2);
    void** cpu_input_refit_emb_fp32_ptr =
        reinterpret_cast<void**>(model_input->cpu_input_refit_tensor.emb_fp32_ptr_tensor.GetPtr<void>());
    int64_t* cpu_input_refit_pos_pair =
        reinterpret_cast<int64_t*>(model_input->cpu_input_refit_tensor.pos_pair_tensor.GetPtr<void>());
    for (size_t i = 0; i < input_refit_size; i++) {
      EXPECT_EQ(cpu_input_refit_emb_fp32_ptr[i], input_refit_emb_ptr[i]);
      EXPECT_EQ(cpu_input_refit_pos_pair[i * 2], input_refit_pos_pair[i].first);
      EXPECT_EQ(cpu_input_refit_pos_pair[i * 2 + 1], input_refit_pos_pair[i].second);
    }
  };

  // Ensure that torch is imported, so that `THPVariableClass` is not nullptr.
  py::module torch = py::module::import("torch");

  // Test for each selected batch size.
  for (const int batch_size : {1, 3, 4}) {
    input_refit_emb_ptr.clear();
    input_refit_pos_pair.clear();

    std::vector<ForwardRequest*> forward_reqs;
    std::vector<std::unique_ptr<ForwardRequest>> forward_reqs_unique;

    // Reserve memory to avoid memory address being moved.
    std::vector<std::vector<int>> output_tokens;
    std::vector<EmbeddingSlice> embedding_slices;
    forward_reqs.reserve(batch_size);
    forward_reqs_unique.reserve(batch_size);
    output_tokens.reserve(batch_size);
    embedding_slices.reserve(batch_size);

    size_t pos_offset = 0;

    // Construct input refit embeddings.
    for (int i = 0; i < batch_size; i++) {
      auto& forward_req = forward_reqs_unique.emplace_back(std::make_unique<ForwardRequest>());
      const size_t output_tokens_size = std::rand() % 4096 + 10;
      output_tokens.emplace_back(output_tokens_size);
      forward_req->forwarding_tokens = std::make_shared<std::vector<int>>(output_tokens.back());
      EmbeddingSlice embedding_slice;
      const int input_refit_size = std::rand() % 3 + 1;
      for (int j = 0; j < input_refit_size; j++) {
        const size_t embedding_size = std::rand() % output_tokens_size + 1;
        const size_t embedding_start_pos = std::rand() % embedding_size;
        embedding_slice.embeddings.emplace_back(embedding_size);
        embedding_slice.pos.push_back(embedding_start_pos);
        input_refit_emb_ptr.emplace_back(embedding_slice.embeddings.back().data());
        input_refit_pos_pair.emplace_back(pos_offset + embedding_start_pos, embedding_size);
      }
      embedding_slices.push_back(std::move(embedding_slice));
      forward_req->input_refit_embedding = &embedding_slices.back();
      forward_reqs.emplace_back(forward_req.get());
      pos_offset += output_tokens_size;
    }

    // Parse and load the input refit embeddings.
    model_input->PrepareInputRefit(forward_reqs);

    // Check the result of PrepareInputRefit.
    VerifyPrepareInputRefit();

    // Construct input refit embedding tensors.
    input_refit_emb_ptr.clear();
    for (int i = 0; i < batch_size; i++) {
      auto& forward_req = forward_reqs[i];
      auto& embedding_slice = forward_req->input_refit_embedding;
      embedding_slice->embedding_tensors.reserve(embedding_slice->embeddings.size());
      for (const auto& embedding : embedding_slice->embeddings) {
        torch::Tensor embedding_tensor = torch::randn(static_cast<int64_t>(embedding.size()), torch::kFloat32);
        input_refit_emb_ptr.push_back(reinterpret_cast<float*>(embedding_tensor.data_ptr()));
        {
          py::gil_scoped_acquire acquire;
          embedding_slice->embedding_tensors.push_back(
              py::reinterpret_steal<py::object>(THPVariable_Wrap(embedding_tensor)));
        }
      }
      embedding_slice->embeddings.clear();
    }

    // Parse and load the input refit embeddings.
    model_input->PrepareInputRefit(forward_reqs);

    // Check the result of PrepareInputRefit.
    VerifyPrepareInputRefit();

    // Construct bad input.
    forward_reqs[0]->input_refit_embedding->embedding_tensors.clear();
    EXPECT_THROW(
        try { model_input->PrepareInputRefit(forward_reqs); } catch (const std::runtime_error& e) {
          EXPECT_NE(strstr(e.what(),
                           "`input_refit_pos.size()` should be equal to `input_refit_embeddings.size()` or "
                           "`input_refit_embedding_tensors.size()`."),
                    nullptr);
          throw;
        },
        std::runtime_error);
  }
}

TEST_F(ModelInputTest, PrepareCutoffLayerTest) {
  model_input->model_config_.type = "minicpm";
  model_input->cutoff_layer = 123;
  auto forward_req = std::make_unique<ForwardRequest>();
  std::vector<ForwardRequest*> forward_reqs_null{forward_req.get()};
  forward_reqs_null[0]->request_target = nullptr;
  model_input->PrepareCutoffLayer(forward_reqs_null);
  EXPECT_EQ(model_input->cutoff_layer, 123);

  model_input->cutoff_layer = 0;
  ksana_llm::TargetDescribe target_desc_empty;
  target_desc_empty.cutoff_layer.clear();
  std::map<std::string, ksana_llm::TargetDescribe> targets_empty = {{"lm_head", target_desc_empty}};
  auto req_targets_empty = std::make_shared<std::map<std::string, ksana_llm::TargetDescribe>>(targets_empty);
  auto req_empty = std::make_unique<ForwardRequest>();
  req_empty->request_target = req_targets_empty.get();
  std::vector<ForwardRequest*> forward_reqs_empty = {req_empty.get()};
  model_input->model_config_.num_layer = 42;
  model_input->PrepareCutoffLayer(forward_reqs_empty);
  EXPECT_EQ(model_input->cutoff_layer, 42);

  model_input->cutoff_layer = 0;
  ksana_llm::TargetDescribe target_desc;
  target_desc.cutoff_layer = {3, 7, 5};
  std::map<std::string, ksana_llm::TargetDescribe> targets = {{"lm_head", target_desc}};
  auto req_targets = std::make_shared<std::map<std::string, ksana_llm::TargetDescribe>>(targets);
  auto req = std::make_unique<ForwardRequest>();
  req->request_target = req_targets.get();
  std::vector<ForwardRequest*> forward_reqs = {req.get()};
  model_input->PrepareCutoffLayer(forward_reqs);
  EXPECT_EQ(model_input->cutoff_layer, 7);
}

TEST_F(ModelInputTest, PrepareUseCacheTest) {
  // Dummy tokens
  std::vector<int> forwarding_tokens = {1, 2, 3};
  SamplingConfig sampling_config1, sampling_config2;
  sampling_config1.max_new_tokens = 1;
  sampling_config2.max_new_tokens = 2;
  // Construct forward requests as test input.
  auto forward_req1 = std::make_unique<ForwardRequest>();
  auto forward_req2 = std::make_unique<ForwardRequest>();
  forward_req1->forwarding_tokens = std::make_shared<std::vector<int>>(forwarding_tokens);
  forward_req2->forwarding_tokens = std::make_shared<std::vector<int>>(forwarding_tokens);
  forward_req1->sampling_config = &sampling_config1;
  forward_req2->sampling_config = &sampling_config2;

  const auto& env = Singleton<Environment>::GetInstance();
  CacheManagerConfig cache_manager_config;
  env->GetCacheManagerConfig(cache_manager_config);

  // Test case 1: All the caching is disabled and all the requests only require the next token.
  RuntimeConfig runtime_config;
  env->GetRuntimeConfig(runtime_config);
  EXPECT_FALSE(runtime_config.enable_prefix_caching);
  EXPECT_FALSE(runtime_config.enable_flexible_caching);
  model_input->PrepareInputInfo({forward_req1.get()});
  model_input->PrepareUseCache(model_input->flash_input);
  EXPECT_FALSE(model_input->use_cache);

  // Test case 2: All the caching is disabled but some requests require more than one token.
  model_input->PrepareInputInfo({forward_req1.get(), forward_req2.get()});
  model_input->PrepareUseCache(model_input->flash_input);
  EXPECT_TRUE(model_input->use_cache);

  // Test case 3: Prefix caching is enabled.
  cache_manager_config.enable_prefix_caching = true;
  env->SetCacheManagerConfig(cache_manager_config);
  env->UpdateModelConfig();
  env->GetRuntimeConfig(runtime_config);
  EXPECT_TRUE(runtime_config.enable_prefix_caching);
  EXPECT_FALSE(runtime_config.enable_flexible_caching);

  model_input->runtime_config_ = runtime_config;  // TODO(robertyuan): ugly, maybe bad test
  model_input->PrepareInputInfo({forward_req1.get()});
  model_input->PrepareUseCache(model_input->flash_input);
  EXPECT_TRUE(model_input->use_cache);

  // Test case 4: Flexible caching is enabled.
  cache_manager_config.enable_prefix_caching = false;
  cache_manager_config.min_flexible_cache_num = 256;
  env->SetCacheManagerConfig(cache_manager_config);
  env->UpdateModelConfig();
  env->GetRuntimeConfig(runtime_config);
  EXPECT_FALSE(runtime_config.enable_prefix_caching);
  EXPECT_TRUE(runtime_config.enable_flexible_caching);

  model_input->runtime_config_ = runtime_config;  // TODO(robertyuan): ugly, maybe bad test
  model_input->PrepareInputInfo({forward_req1.get()});
  model_input->PrepareUseCache(model_input->flash_input);
  EXPECT_TRUE(model_input->use_cache);
}

#ifdef ENABLE_CUDA
// Test PrepareFlashRotary with flexible caching
TEST_F(ModelInputTest, PrepareFlashRotaryMlaFlexibleCacheTest) {
  // Dummy input req
  const int block_token_num = 2;
  const int batch_size = 2;
  std::vector<int> dummy_block_memory_ids(1, 0);
  std::vector<int> forwarding_tokens_1 = {0, 1, 2, 4, 5, 6, 7};     // hit {0, 1, 2, 3, 4, 5}
  std::vector<int> forwarding_tokens_2 = {10, 11, 14, 15, 16, 17};  // hit {10, 11, 12, 13, 14, 15}
  std::vector<FlexibleCachedCopyTask> flexible_cached_copy_tasks_1(3);
  std::vector<FlexibleCachedCopyTask> flexible_cached_copy_tasks_2(2);
  flexible_cached_copy_tasks_1[0].Update(2, 2, dummy_block_memory_ids, dummy_block_memory_ids);
  flexible_cached_copy_tasks_1[1].Update(3, 4, dummy_block_memory_ids, dummy_block_memory_ids);
  flexible_cached_copy_tasks_1[2].Update(4, 5, dummy_block_memory_ids, dummy_block_memory_ids);
  flexible_cached_copy_tasks_2[0].Update(2, 4, dummy_block_memory_ids, dummy_block_memory_ids);
  flexible_cached_copy_tasks_2[1].Update(3, 5, dummy_block_memory_ids, dummy_block_memory_ids);
  // Construct forward requests as test input.
  auto forward_req_1 = std::make_unique<ForwardRequest>();
  auto forward_req_2 = std::make_unique<ForwardRequest>();
  forward_req_1->forwarding_tokens = std::make_shared<std::vector<int>>(forwarding_tokens_1);
  forward_req_2->forwarding_tokens = std::make_shared<std::vector<int>>(forwarding_tokens_2);
  forward_req_1->kv_cached_token_num = 2;
  forward_req_2->kv_cached_token_num = 2;
  forward_req_1->prefix_cache_len = forward_req_1->kv_cached_token_num + flexible_cached_copy_tasks_1.size();
  forward_req_2->prefix_cache_len = forward_req_2->kv_cached_token_num + flexible_cached_copy_tasks_2.size();
  forward_req_1->flexible_cached_copy_tasks = &flexible_cached_copy_tasks_1;
  forward_req_2->flexible_cached_copy_tasks = &flexible_cached_copy_tasks_2;

  const size_t total_tokens = forwarding_tokens_1.size() + forwarding_tokens_2.size();
  const size_t total_prefix_len = forward_req_1->prefix_cache_len + forward_req_2->prefix_cache_len;
  const size_t total_flexible_len = flexible_cached_copy_tasks_1.size() + flexible_cached_copy_tasks_2.size();

  const auto& env = Singleton<Environment>::GetInstance();
  CacheManagerConfig cache_manager_config;
  env->GetCacheManagerConfig(cache_manager_config);
  cache_manager_config.enable_prefix_caching = true;
  cache_manager_config.min_flexible_cache_num = block_token_num;
  env->SetCacheManagerConfig(cache_manager_config);
  env->UpdateModelConfig();

  RuntimeConfig runtime_config;
  env->GetRuntimeConfig(runtime_config);
  EXPECT_TRUE(runtime_config.enable_prefix_caching);
  EXPECT_TRUE(runtime_config.enable_flexible_caching);

  EXPECT_TRUE(model_input->model_config_.use_mla);
  EXPECT_TRUE(model_input->enable_blocked_multi_token_forwarding_kv_);
  model_input->dp_total_prefix_len = total_prefix_len;
  model_input->dp_dst_flexible_kv_cache_tensor.shape = {total_flexible_len};
  model_input->runtime_config_ = runtime_config;
  std::vector<ForwardRequest*> forward_reqs = {forward_req_1.get(), forward_req_2.get()};
  model_input->PrepareInputInfo(forward_reqs);
  EXPECT_EQ(model_input->flash_input.dp_reqs.size(), forward_reqs.size());
  model_input->PrepareFlashRotary(model_input->flash_input);

  // verify rotary_embedding_pos and rotary_embedding_mask
  std::vector<int64_t> host_rotary_embedding_pos(total_tokens - total_prefix_len);
  std::vector<int64_t> host_rotary_embedding_mask(total_tokens - total_prefix_len);
  std::vector<int64_t> host_src_flexible_rotary_embedding_pos(total_tokens);
  std::vector<int64_t> host_dst_flexible_rotary_embedding_pos(total_tokens);
  std::vector<int64_t> host_flexible_rotary_embedding_mask(total_tokens);

  Memcpy(host_rotary_embedding_pos.data(), model_input->flash_input.rotary_embedding_pos.GetPtr<void>(),
         (total_tokens - total_prefix_len) * sizeof(int64_t), MEMCPY_DEVICE_TO_HOST);
  Memcpy(host_rotary_embedding_mask.data(), model_input->flash_input.rotary_embedding_mask.GetPtr<void>(),
         (total_tokens - total_prefix_len) * sizeof(int64_t), MEMCPY_DEVICE_TO_HOST);
  Memcpy(host_src_flexible_rotary_embedding_pos.data(),
         model_input->dp_src_flexible_rotary_embedding_pos.GetPtr<void>(), total_tokens * sizeof(int64_t),
         MEMCPY_DEVICE_TO_HOST);
  Memcpy(host_dst_flexible_rotary_embedding_pos.data(),
         model_input->dp_dst_flexible_rotary_embedding_pos.GetPtr<void>(), total_tokens * sizeof(int64_t),
         MEMCPY_DEVICE_TO_HOST);
  Memcpy(host_flexible_rotary_embedding_mask.data(), model_input->dp_flexible_rotary_embedding_mask.GetPtr<void>(),
         total_tokens * sizeof(int64_t), MEMCPY_DEVICE_TO_HOST);

  int rope_idx = 0;
  int flexible_rope_idx = 0;
  for (int i = 0; i < batch_size; ++i) {
    int context_forwarding_token_num = forward_reqs[i]->forwarding_tokens->size() - forward_reqs[i]->prefix_cache_len;
    for (int idx = 0; idx < context_forwarding_token_num; ++idx) {
      EXPECT_EQ(host_rotary_embedding_pos[rope_idx], idx + forward_reqs[i]->prefix_cache_len);
      EXPECT_EQ(host_rotary_embedding_mask[rope_idx], 1);
      rope_idx++;
    }

    for (int idx = 0; idx < forward_reqs[i]->flexible_cached_copy_tasks->size(); ++idx) {
      auto& task = forward_reqs[i]->flexible_cached_copy_tasks->at(idx);
      EXPECT_EQ(host_src_flexible_rotary_embedding_pos[flexible_rope_idx + task.dst_token_idx_], task.src_token_idx_);
      EXPECT_EQ(host_dst_flexible_rotary_embedding_pos[flexible_rope_idx + task.dst_token_idx_], task.dst_token_idx_);
    }

    for (int idx = 0; idx < forward_reqs[i]->forwarding_tokens->size(); ++idx) {
      if (idx >= forward_reqs[i]->kv_cached_token_num && idx < forward_reqs[i]->prefix_cache_len) {
        EXPECT_EQ(host_flexible_rotary_embedding_mask[flexible_rope_idx], 1);
      } else {
        EXPECT_EQ(host_flexible_rotary_embedding_mask[flexible_rope_idx], 0);
      }
      flexible_rope_idx++;
    }
  }
}
#endif

#ifdef ENABLE_CUDA
TEST_F(ModelInputTest, PrepareFlashMlaTest) {
  model_input->page_inputs.clear();
  auto& page_input = model_input->page_inputs.emplace_back();
  page_input.q_seq_len = 1;
  // Init shared_tensors in page_input
  page_input.input_length = model_input->input_length;
  model_input->num_splits.shape = {0};

  // 测试用例1: 当model_config_.use_mla为false时，PrepareFlashMla应该直接返回
  model_input->model_config_.use_mla = false;
  model_input->single_token_request_num = 5;
  model_input->PrepareFlashMla(page_input);
  // 由于方法直接返回，没有明确的状态变化可以验证，这里我们只是确保方法不会崩溃

  // 测试用例2: 当page_input.dp_reqs为空时，PrepareFlashMla应该直接返回
  model_input->model_config_.use_mla = true;
  model_input->PrepareFlashMla(page_input);
  // 同样，这里我们只是确保方法不会崩溃

  // 测试用例3: 当所有条件满足时，PrepareFlashMla应该执行相应操作
  // 准备测试数据
  model_input->model_config_.use_mla = true;
  model_input->model_config_.head_num = 16;
  model_input->runtime_config_.parallel_basic_config.tensor_parallel_size = 1;
  page_input.dp_reqs.resize(4);

  // 创建输入长度张量
  std::vector<int> input_lengths = {0, 20, 30, 40, 50};
  MemcpyAsync(page_input.input_length.GetPtr<void>(), input_lengths.data(), input_lengths.size() * sizeof(int),
              MEMCPY_HOST_TO_DEVICE, model_input->context_->GetH2DStreams()[model_input->rank_]);

  // 执行PrepareFlashMla
  model_input->PrepareFlashMla(page_input);

  // 从GPU复制数据回CPU并打印
  llm_kernels::nvidia::FlashMlaWorkspaceMap flash_mla_workspace_map;
  GetNumSmParts(flash_mla_workspace_map, 16, 1, 0);
  if (flash_mla_workspace_map.num_sm_parts > 1) {
    int num_splits_cpu;

    MemcpyAsync(&num_splits_cpu, page_input.num_splits.GetPtr<void>(), sizeof(int), MEMCPY_DEVICE_TO_HOST,
                model_input->context_->GetH2DStreams()[model_input->rank_]);

    // 同步流以确保复制完成
    StreamSynchronize(model_input->context_->GetH2DStreams()[model_input->rank_]);
    EXPECT_EQ(num_splits_cpu, 0);
  }
}
#endif

#ifdef ENABLE_CUDA
TEST_F(ModelInputTest, PrepareNextNGatherIdxTest) {
  auto RandomNum = [](const size_t min, const size_t max) {
    static std::default_random_engine random_engine;
    return std::uniform_int_distribution<size_t>(min, max)(random_engine);
  };

  constexpr size_t kReqNum = 10;
  constexpr size_t kMaxReqLen = 1024;

  std::vector<std::unique_ptr<ForwardRequest>> forward_unique_ptrs;
  std::vector<ForwardRequest*> forward_reqs(kReqNum);
  std::vector<std::vector<int>> req_tokens(kReqNum);
  for (size_t i = 0; i < kReqNum; ++i) {
    forward_unique_ptrs.emplace_back(std::make_unique<ForwardRequest>());
    forward_reqs[i] = forward_unique_ptrs.back().get();
    req_tokens[i].resize(RandomNum(0, kMaxReqLen));
    forward_reqs[i]->forwarding_tokens = std::make_shared<std::vector<int>>(req_tokens[i]);
    forward_reqs[i]->kv_cached_token_num = RandomNum(0, req_tokens[i].size());
    forward_reqs[i]->req_id = i;
  }

  model_input->mtp_req_id_to_pos_.clear();
  model_input->PrepareNextNGatherIdx(forward_reqs, RunMode::kMain);

  EXPECT_EQ(model_input->mtp_req_id_to_pos_.size(), forward_reqs.size());
  size_t total_len = 0;
  for (size_t i = 0; i < forward_reqs.size(); ++i) {
    EXPECT_EQ(total_len, model_input->mtp_req_id_to_pos_[forward_reqs[i]->req_id]);
    total_len += forward_reqs[i]->forwarding_tokens->size() - forward_reqs[i]->kv_cached_token_num;
  }

  model_input->PrepareNextNGatherIdx(forward_reqs, RunMode::kNextN);
  EXPECT_EQ(model_input->nextn_hidden_idx_uint64_tensor.shape.size(), 1);
  EXPECT_EQ(model_input->nextn_hidden_idx_uint64_tensor.shape[0], total_len);

  std::vector<size_t> host_idx_result(total_len);
  Memcpy(host_idx_result.data(), model_input->nextn_hidden_idx_uint64_tensor.GetPtr<void>(), total_len * sizeof(size_t),
         MEMCPY_DEVICE_TO_HOST);
  size_t result_i = 0, counter_i = 0;
  for (size_t i = 0; i < forward_reqs.size(); ++i) {
    const auto& req = *forward_reqs[i];
    for (size_t token_i = 0; token_i < req.forwarding_tokens->size(); ++token_i) {
      if (token_i < static_cast<size_t>(req.kv_cached_token_num)) {
        continue;
      }
      EXPECT_EQ(host_idx_result[result_i++], counter_i++);
    }
  }
  EXPECT_EQ(host_idx_result.size(), result_i);
}
#endif

TEST_F(ModelInputTest, PrepareMRopePosTest) {
  // Set the model type to qwen2_vl to enable MRoPE tensor creation
  model_input->model_config_.type = "qwen2_vl";
  model_input->model_config_.rope_scaling_factor_config.mrope_section = std::vector<int>{16, 24, 24};
  model_input->CreateVLTensors();

  auto VerifyPrepareMRopePos = [&](const std::vector<ForwardRequest*>& forward_reqs,
                                   const std::vector<int64_t>& expected_mrotary_embedding_pos,
                                   const std::vector<int64_t>& expected_offsets) {
    EXPECT_EQ(model_input->dp_mrotary_embedding_pos.shape.size(), 2);
    EXPECT_GE(model_input->dp_mrotary_embedding_pos.shape[1], expected_mrotary_embedding_pos.size());

    // Verify the offsets.
    EXPECT_EQ(expected_offsets.size(), forward_reqs.size());
    for (size_t i = 0; i < forward_reqs.size(); i++) {
      EXPECT_EQ(*forward_reqs[i]->mrotary_embedding_pos_offset, expected_offsets[i]);
    }

    // Verify the mrotary_embedding_pos tensor.
    const size_t total_pos_count = expected_mrotary_embedding_pos.size();
    std::vector<int64_t> actual_mrotary_embedding_pos(total_pos_count);
    Memcpy(actual_mrotary_embedding_pos.data(), model_input->dp_mrotary_embedding_pos.GetPtr<void>(),
           total_pos_count * sizeof(int64_t), MEMCPY_DEVICE_TO_HOST);
    for (size_t i = 0; i < total_pos_count; i++) {
      EXPECT_EQ(actual_mrotary_embedding_pos[i], expected_mrotary_embedding_pos[i]);
    }
  };

  // Ensure that torch is imported for tensor handling
  py::module torch = py::module::import("torch");

  // Test for each selected batch size.
  for (const int batch_size : {1, 3, 4}) {
    std::vector<ForwardRequest*> forward_reqs;
    std::vector<std::unique_ptr<ForwardRequest>> forward_reqs_unique;
    std::vector<std::vector<int>> output_tokens;
    std::vector<EmbeddingSlice> embedding_slices;
    std::vector<int64_t> mrotary_offsets;

    // Reserve memory to avoid memory address being moved.
    forward_reqs.reserve(batch_size);
    output_tokens.reserve(batch_size);
    embedding_slices.reserve(batch_size);
    mrotary_offsets.reserve(batch_size);

    std::vector<int64_t> expected_mrotary_embedding_pos;
    std::vector<int64_t> expected_offsets;

    // Create a mix of plain text and visual inputs
    for (int i = 0; i < batch_size; i++) {
      auto& forward_req = forward_reqs_unique.emplace_back(std::make_unique<ForwardRequest>());
      const size_t token_size = 10 + i * 5;
      output_tokens.emplace_back(token_size);
      forward_req->forwarding_tokens = std::make_shared<std::vector<int>>(output_tokens.back());
      EmbeddingSlice embedding_slice;
      embedding_slices.push_back(std::move(embedding_slice));
      forward_req->input_refit_embedding = &embedding_slices.back();
      mrotary_offsets.push_back(0);
      forward_req->mrotary_embedding_pos_offset = &mrotary_offsets.back();

      // Alternate between plain text and visual input
      if (i % 2 == 0) {
        // Plain text input (empty additional_tensors)
        // For plain text, the function creates positions where each triplet is [i, i, i]
        int64_t list_size = forward_req->forwarding_tokens->size() * 3;
        for (int64_t j = 0; j < list_size; j += 3) {
          expected_mrotary_embedding_pos.push_back(j);
          expected_mrotary_embedding_pos.push_back(j);
          expected_mrotary_embedding_pos.push_back(j);
        }
        expected_offsets.push_back(0);
      } else {
        // Visual input (non-empty additional_tensors)
        // Create position tensor with deterministic values for testing
        torch::Tensor pos_tensor = torch::randint(0, 100, {static_cast<int64_t>(token_size * 3)}, torch::kInt64);
        int64_t offset_value = i * 10;
        torch::Tensor offset_tensor = torch::tensor(offset_value, torch::kInt64);
        auto pos_accessor = pos_tensor.data_ptr<int64_t>();
        for (int64_t j = 0; j < pos_tensor.numel(); j++) {
          expected_mrotary_embedding_pos.push_back(pos_accessor[j]);
        }
        expected_offsets.push_back(offset_value);
        // Add tensors to additional_tensors
        {
          py::gil_scoped_acquire acquire;
          forward_req->input_refit_embedding->additional_tensors.push_back(
              py::reinterpret_steal<py::object>(THPVariable_Wrap(pos_tensor)));
          forward_req->input_refit_embedding->additional_tensors.push_back(
              py::reinterpret_steal<py::object>(THPVariable_Wrap(offset_tensor)));
        }
      }
      forward_reqs.push_back(forward_req.get());
    }

    // Parse MRopePos.
    model_input->PrepareMRopePos(forward_reqs);

    // Check the result of VerifyPrepareMRopePos.
    VerifyPrepareMRopePos(forward_reqs, expected_mrotary_embedding_pos, expected_offsets);
  }

  // Construct bad input.
  {
    auto forward_req = std::make_unique<ForwardRequest>();
    std::vector<ForwardRequest*> forward_reqs{forward_req.get()};
    std::vector<std::vector<int>> output_tokens(1, std::vector<int>(10));
    std::vector<EmbeddingSlice> embedding_slices(1);
    std::vector<int64_t> mrotary_offsets(1, 0);

    forward_reqs[0]->forwarding_tokens = std::make_shared<std::vector<int>>(output_tokens[0]);
    forward_reqs[0]->input_refit_embedding = &embedding_slices[0];
    forward_reqs[0]->mrotary_embedding_pos_offset = &mrotary_offsets[0];

    {
      torch::Tensor pos_tensor = torch::randint(0, 100, {30}, torch::kInt64);
      {
        py::gil_scoped_acquire acquire;
        forward_reqs[0]->input_refit_embedding->additional_tensors.clear();
        forward_reqs[0]->input_refit_embedding->additional_tensors.push_back(
            py::reinterpret_steal<py::object>(THPVariable_Wrap(pos_tensor)));
      }

      EXPECT_THROW(
          try { model_input->PrepareMRopePos(forward_reqs); } catch (const std::runtime_error& e) {
            EXPECT_NE(strstr(e.what(), "additional_tensors should contain at least 2 tensors"), nullptr);
            throw;
          },
          std::runtime_error);
    }
  }
}

TEST_F(ModelInputTest, PrepareUseGreedyTest) {
#ifdef ENABLE_CUDA
  SamplingConfig sampling_config1;
  auto forward_req1 = std::make_unique<ForwardRequest>();
  forward_req1->sampling_config = &sampling_config1;
  auto forward_req2 = std::make_unique<ForwardRequest>();
  SamplingConfig sampling_config2;
  forward_req2->sampling_config = &sampling_config2;
  std::vector<ForwardRequest*> forward_reqs{forward_req1.get(), forward_req2.get()};
  // Use greedy sampler by default
  model_input->PrepareUseGreedy(forward_reqs);
  EXPECT_TRUE(model_input->use_greedy);

  // Greedy sampler is disabled when using topk
  forward_req2->sampling_config->topk = 2;
  model_input->PrepareUseGreedy(forward_reqs);
  EXPECT_FALSE(model_input->use_greedy);
  forward_req2->sampling_config->topk = 1;

  // Greedy sampler is disabled when requiring logits
  forward_req2->logits_custom_length = 1;
  model_input->PrepareUseGreedy(forward_reqs);
  EXPECT_FALSE(model_input->use_greedy);
  forward_req2->logits_custom_length = 0;

  // Greedy sampler is disabled when using xgrammar
  model_input->batch_scheduler_config_.enable_xgrammar = true;
  model_input->PrepareUseGreedy(forward_reqs);
  EXPECT_FALSE(model_input->use_greedy);
  model_input->batch_scheduler_config_.enable_xgrammar = false;
#endif
}

}  // namespace ksana_llm
