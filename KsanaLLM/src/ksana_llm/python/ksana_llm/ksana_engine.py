# Copyright 2024 Tencent Inc.  All rights reserved.
#
# ==============================================================================
import asyncio
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple, Union

import libtorch_serving
from transformers import GenerationConfig, PreTrainedTokenizerFast
from utilize.logger import get_logger
from .arg_utils import EngineArgs
from .ksana_plugin import KsanaPlugin, PluginConfig
from .processor_op_base import TokenizerProcessorOpBase


logger = get_logger(__name__)

# Adjust the max_workers to a reasonable number based on the number of CPUs
# Give threads a readable name for easier debugging.
model_executor = ThreadPoolExecutor(
    max_workers=1024, thread_name_prefix="ksana-engine"
)


@dataclass
class EndpointConfig:
    endpoint: str = "python"  # endpoint type
    host: str = "0.0.0.0"  # endpoint host address
    port: int = 8080  # endpoint port
    access_log: bool = True  # whether to enable the endpoint access log


@dataclass
class ReasoningConfig:
    """Configuration for reasoning models that require special handling during inference.

    This config is designed for models with reasoning capabilities like DeepSeek-R1
    that need to disable constrained decoding during the thinking/reasoning phase.

    Attributes:
        think_end_token_id: Token ID marking the end of reasoning phase. When this token
                           is encountered, the model will resume normal constrained decoding.

    Future extensibility examples:
        - max_think_tokens: maximum number of tokens allowed in the reasoning phase.
    """
    think_end_token_id: Optional[int] = None


class PyAsyncStreamingIterator:
    """The streaming iterator."""

    def __init__(
        self,
        serving_iterator: libtorch_serving.StreamingIterator,
        ksana_plugin: KsanaPlugin,
        ksana_python_input: libtorch_serving.KsanaPythonInput,
    ):
        self._serving_iterator = serving_iterator
        self._ksana_plugin = ksana_plugin
        self._ksana_python_input = ksana_python_input

    def __aiter__(self):
        return self

    # Define an asynchronous iterator method
    async def __anext__(self):
        # Get the current event loop
        loop = asyncio.get_running_loop()

        # Run the GetNext method of the serving iterator in an executor
        status, ksana_python_output = await loop.run_in_executor(
            model_executor, self._serving_iterator.GetNext
        )

        # Check the status of the iteration
        if status.OK():
            # If the iteration is successful, return the output
            self._ksana_plugin.postprocess(
                self._ksana_python_input, ksana_python_output
            )
            return ksana_python_output
        else:
            # If the iteration has finished, raise a StopAsyncIteration exception
            raise StopAsyncIteration(
                f"Iterator finished, ret code {status.GetCode()}, message {status.GetMessage()}."
            )


class KsanaLLMEngine:
    """The LLM serving model instance."""

    def __init__(
        self,
        config_file: str,
        plugin_config: PluginConfig,
        endpoint_config: EndpointConfig,
        pre_post_processor: TokenizerProcessorOpBase,
    ):
        """Initialize a KsanaLLMEngine instance.

        Args:
            config_file: The serving config file.
            plugin_config: Configuration for the Ksana plugin.
            endpoint_config: Configuration for the endpoint.
            pre_post_processor: The pre/post processor that contains the tokenizer.
        """
        self._config_file = config_file

        # The serving instance.
        self._serving = libtorch_serving.Serving()

        self._ksana_plugin = KsanaPlugin(plugin_config)

        if endpoint_config.endpoint != "python":
            self._serving.endpoint_config.type = libtorch_serving.EndpointType.RPC
            self._serving.endpoint_config.rpc_plugin_name = endpoint_config.endpoint

        self._serving.endpoint_config.host = endpoint_config.host
        self._serving.endpoint_config.port = endpoint_config.port
        self._serving.endpoint_config.access_log = endpoint_config.access_log

        self.pre_post_processor = pre_post_processor
        self.tokenizer = self.pre_post_processor.tokenizer

        if not isinstance(self.tokenizer, PreTrainedTokenizerFast):
            print(
                "Using a slow tokenizer. This might cause a significant "
                "slowdown. Consider using a fast tokenizer instead."
            )

        self.eos_token_id = None
        if hasattr(self.tokenizer, "eos_token_id"):
            self.eos_token_id = self.tokenizer.eos_token_id

    def initialize(self, reasoning_config: Optional[ReasoningConfig] = None) -> None:
        """Initialize the model.

        Args:
            reasoning_config: Optional configuration for reasoning models.
        """
        self._serving.init_serving(self._config_file, reasoning_config)

    @classmethod
    def from_engine_args(cls, args: EngineArgs) -> "KsanaLLMEngine":
        """Create an instance from EngineArgs."""
        if args.config_file is None:
            raise ValueError("config_file is required.")
        plugin_config = PluginConfig(
            model_dir=args.model_dir,
            config_file=args.config_file,
            plugin_name=args.model_type,
            enable_trt=args.plugin_model_enable_trt,
            thread_pool_size=args.plugin_thread_pool_size,
            vit_model_type=args.multi_modal_vit_model_type
        )

        endpoint_config = EndpointConfig(
            endpoint=args.endpoint,
            host=args.host,
            port=args.port,
            access_log=args.access_log,
        )

        pre_post_processor = TokenizerProcessorOpBase(
            args.model_dir,
            args.tokenizer_path,
            args.tokenization
        )

        return cls(
            config_file=args.config_file,
            plugin_config=plugin_config,
            endpoint_config=endpoint_config,
            pre_post_processor=pre_post_processor,
        )

    def _build_generator_config(self, sampling_config: dict) -> GenerationConfig:
        """
        Build the transformers GenerationConfig object Validate the 
        values of the attributes.
        """
        def _get_sampling_value(sampling_config: dict, key: str, default_val=None):
            """Get value from sampling_config dict, return default if key not exists.
            """
            return sampling_config[key] if key in sampling_config else default_val

        stop_token_ids = _get_sampling_value(sampling_config, "stop_token_ids", None)
        if not stop_token_ids:
            stop_token_ids = self.pre_post_processor.get_stop_token_ids()
        ignore_eos = _get_sampling_value(sampling_config, "ignore_eos", False)

        if (
            self.pre_post_processor.get_tokenizer_stop_token_ids() is not None
            and not ignore_eos
        ):
            eos_token_id = self.pre_post_processor.get_tokenizer_stop_token_ids()
            if isinstance(eos_token_id, list):
                for token_id in eos_token_id:
                    if token_id not in stop_token_ids:
                        stop_token_ids.append(token_id)
            else:
                if eos_token_id not in stop_token_ids:
                    stop_token_ids.append(eos_token_id)

        generation_config = GenerationConfig(
            top_k=_get_sampling_value(sampling_config, "topk", 1),
            do_sample=_get_sampling_value(sampling_config, "do_sample", None),
            top_p=_get_sampling_value(sampling_config, "topp", 1.0),
            temperature=_get_sampling_value(sampling_config, "temperature", 1.0),
            logprobs_num=_get_sampling_value(sampling_config, "logprobs", 0),
            repetition_penalty=_get_sampling_value(sampling_config,
                                                "repetition_penalty", 1.0),
            no_repeat_ngram_size=_get_sampling_value(sampling_config,
                                                "no_repeat_ngram_size", 0),
            encoder_no_repeat_ngram_size=_get_sampling_value(sampling_config,
                                                "encoder_no_repeat_ngram_size", 0),
            decoder_no_repeat_ngram_size=_get_sampling_value(sampling_config,
                                                "decoder_no_repeat_ngram_size", 0),
            enable_structured_output=_get_sampling_value(sampling_config,
                                                "enable_structured_output", False),
            json_schema=_get_sampling_value(sampling_config,
                                                "json_schema", ""),
            num_beams=_get_sampling_value(sampling_config,
                                                "num_beams", 1),
            num_return_sequences=_get_sampling_value(sampling_config,
                                                "num_return_sequences", 1),
            length_penalty=_get_sampling_value(sampling_config,
                                                "length_penalty", 1.0),
            stop_strings=_get_sampling_value(sampling_config,
                                                "stop_strings", []),
            enable_thinking=_get_sampling_value(sampling_config,
                                                "enable_thinking", True),
            stop_token_ids=stop_token_ids,
            ignore_eos=ignore_eos
        )
        if "max_new_tokens" in sampling_config:
            generation_config.max_new_tokens = sampling_config["max_new_tokens"]

        return generation_config

    def _build_python_input(
        self,
        request_dict: Dict[str, Any] = None,
        generation_config: GenerationConfig = None,
    ) -> libtorch_serving.KsanaPythonInput:
        """Build the KsanaPythonInput object."""


        prompt_text: Optional[str] = request_dict.pop("prompt", None)
        input_tokens = request_dict.pop("input_tokens", None)

        # `messages` is the OpenAI Chat Completion API that can contain visual input
        # `additonal_params` are model specific params packed in a dict, e.g.,
        # `max_pixels`, `fps` for imgs and videos
        messages: Optional[List[Dict]] = request_dict.pop("messages", None)
        use_chat_template = request_dict.pop("use_chat_template", False)

        if input_tokens is None and prompt_text:
            model_type = request_dict.get("model_type", "empty")
            prompt_text = self.pre_post_processor.build_prompt(prompt_text, model_type, use_chat_template)
            input_tokens = self.pre_post_processor.encode(prompt_text)

        # Create a KsanaPythonInput object
        ksana_python_input = libtorch_serving.KsanaPythonInput()
        ksana_python_input.model_name = request_dict.pop("model_name", "")

        # Set input tokens to the ksana_python_input
        if input_tokens is not None:
            ksana_python_input.input_tokens = input_tokens
            if(request_dict.get("debug_mode", False)):
                logger.info(f"[DEBUG] Input tokens: {input_tokens}")
                decoded_text = self.pre_post_processor.decode(input_tokens, is_stream_generate=False)
                logger.info(f"[DEBUG] Input tokens decoded: {decoded_text}")
        # Set the generation parameters to the ksana_python_input sampling_config
        sampling_config = ksana_python_input.sampling_config

        def _get_generation_value(
            generation_config: GenerationConfig, key: str, default_val
        ):
            """Get value from generation_config, return default if key not exists.
            """
            value = getattr(generation_config, key, default_val)
            return default_val if value is None else value

        sampling_config.num_beams = _get_generation_value(
            generation_config, "num_beams", 1
        )
        sampling_config.topk = _get_generation_value(generation_config, "top_k", 1)
        sampling_config.topp = _get_generation_value(generation_config, "top_p", 1.0)
        sampling_config.temperature = _get_generation_value(
            generation_config, "temperature", 1.0
        )
        sampling_config.max_new_tokens = _get_generation_value(
            generation_config, "max_new_tokens", -1
        )
        sampling_config.logprobs_num = _get_generation_value(
            generation_config, "logprobs_num", 0
        )
        sampling_config.num_return_sequences = _get_generation_value(
            generation_config, "num_return_sequences", 1
        )
        sampling_config.repetition_penalty = _get_generation_value(
            generation_config, "repetition_penalty", 1.0
        )
        sampling_config.no_repeat_ngram_size = _get_generation_value(
            generation_config, "no_repeat_ngram_size", 0
        )
        sampling_config.encoder_no_repeat_ngram_size = _get_generation_value(
            generation_config, "encoder_no_repeat_ngram_size", 0
        )
        sampling_config.decoder_no_repeat_ngram_size = _get_generation_value(
            generation_config, "decoder_no_repeat_ngram_size", 0
        )
        sampling_config.enable_structured_output = _get_generation_value(
            generation_config, "enable_structured_output", False
        )
        sampling_config.json_schema = _get_generation_value(
            generation_config, "json_schema", ""
        )
        sampling_config.length_penalty = _get_generation_value(
            generation_config, "length_penalty", 1.0
        )
        sampling_config.stop_token_ids = _get_generation_value(
            generation_config, "stop_token_ids", []
        )
        sampling_config.ignore_eos = _get_generation_value(
            generation_config, "ignore_eos", False
        )
        sampling_config.stop_strings = _get_generation_value(
            generation_config, "stop_strings", []
        )
        sampling_config.enable_thinking = _get_generation_value(
            generation_config, "enable_thinking", True
        )

        def _check_do_sample_params():
            do_sample = True
            if (
                _get_generation_value(generation_config, "do_sample", None) is False
                or sampling_config.topk == 1
            ):
                do_sample = False

            if (
                sampling_config.topk == 1
                and _get_generation_value(generation_config, "do_sample", None) is True
            ):
                print(
                    "Generation parameter topk cannot be 1 when do_sample is explicitly set to True!"
                )

            if not do_sample:
                sampling_config.topk = 1
                sampling_config.topp = 1.0
                sampling_config.temperature = 1.0

        _check_do_sample_params()

        # Set additional parameters with the request_dict
        if "input_refit_embedding" in request_dict:
            input_refit_embedding = request_dict["input_refit_embedding"]
            if "pos" in input_refit_embedding:
                ksana_python_input.input_refit_embedding.pos = input_refit_embedding[
                    "pos"
                ]
            if "embeddings" in input_refit_embedding:
                ksana_python_input.input_refit_embedding.embeddings = (
                    input_refit_embedding["embeddings"]
                )
        else:
            # Support independent pos and embeddings parameters for faster transmission
            if "pos" in request_dict:
                ksana_python_input.input_refit_embedding.pos = request_dict["pos"]
            if "embeddings" in request_dict:
                ksana_python_input.input_refit_embedding.embeddings = request_dict["embeddings"]
            
        if "structured_output_regex" in request_dict:
            ksana_python_input.structured_output_regex = request_dict[
                "structured_output_regex"
            ]

        plugin_kwargs = {
            "messages": messages,
            "additional_params": request_dict.get("additional_params", {}),
            "prompt": prompt_text,
        }

        # First exec preprocessing of ksana plugin before inference
        self._ksana_plugin.preprocess(ksana_python_input, **plugin_kwargs)

        return ksana_python_input

    def _call_serving_generate(self, ksana_python_input, req_ctx):
        return self._serving.generate(ksana_python_input, req_ctx)

    def _call_serving_generate_streaming(self, ksana_python_input, req_ctx):
        return self._serving.generate_streaming(ksana_python_input, req_ctx)

    def _call_serving_forward(self, request_bytes, req_ctx):
        return self._serving.forward(request_bytes, req_ctx)

    async def generate(
        self,
        request_dict: Dict[str, Any],
        model_name: Optional[str] = None,
        streamer: Optional[bool] = None,
        req_ctx: Optional[Dict[str, str]] = None,
    ) -> Tuple[Any, Union[Dict[str, Any], AsyncGenerator]]:
        """The model generate interface."""
        if req_ctx is None:
            req_ctx = {}

        # Check if plugin is required for generation input
        if (not request_dict.get("input_tokens") and not request_dict.get("prompt")
                and request_dict.get("messages") and not self._ksana_plugin.is_available()):
            status = libtorch_serving.Status(
                libtorch_serving.RetCode.RET_INVALID_ARGUMENT,
                "messages input requires a plugin or use '/v1/chat/completions' API"
            )
            return status, None

        # Build a  transformers generation_config from the request_dict 
        # used to validate the values
        generation_config = self._build_generator_config(request_dict.get("sampling_config", {}))
        # Build the ksana_python_input object
        ksana_python_input = self._build_python_input(request_dict, generation_config)
        loop = asyncio.get_running_loop()
        if streamer:
            status, streaming_iterator = await loop.run_in_executor(
                model_executor,
                self._call_serving_generate_streaming,
                ksana_python_input,
                req_ctx
            )

            if not status.OK():
                return status, None

            return status, PyAsyncStreamingIterator(
                streaming_iterator, self._ksana_plugin, ksana_python_input
            )

        status, ksana_python_output = await loop.run_in_executor(
            model_executor,
            self._call_serving_generate,
            ksana_python_input,
            req_ctx
        )
        if not status.OK():
            return status, None
        self._ksana_plugin.postprocess(ksana_python_input, ksana_python_output)

        return status, ksana_python_output

    async def forward(
        self, request_bytes: bytes, req_ctx: Optional[Dict[str, str]] = None
    ) -> Tuple[Any, Optional[bytes]]:
        """The model forward interface."""
        if req_ctx is None:
            req_ctx = {}
        loop = asyncio.get_running_loop()
        status, response_bytes = await loop.run_in_executor(
            model_executor,
            self._call_serving_forward,
            request_bytes,
            req_ctx
        )
        if status.OK():
            return status, response_bytes
        else:
            return status, None
