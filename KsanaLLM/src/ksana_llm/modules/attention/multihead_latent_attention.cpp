/* Copyright 2024 Tencent Inc.  All rights reserved.

==============================================================================*/

#include "ksana_llm/modules/attention/multihead_latent_attention.h"

#include "ksana_llm/profiler/profile_event.h"
#include "ksana_llm/utils/tensor.h"

namespace ksana_llm {

MultiHeadLatentAttention::MultiHeadLatentAttention(int layer_idx, bool is_neox, LayerCreationContext& creation_context,
                                                   ModelCreationConfig& model_creation_config, MlaBuffers& mla_buffers)
    : layer_idx_(layer_idx),
      tensor_parallel_size_(creation_context.runtime_config.parallel_basic_config.tensor_parallel_size),
      mla_buffers_(mla_buffers) {
  if (creation_context.runtime_config.enable_o_proj_out_of_dp) {
    o_proj_out_of_dp_ = true;
    KLLM_LOG_DEBUG << "Enable o_proj_out_of_dp";
  }

  auto& attn_config = model_creation_config.attn_config;
  use_dsa_ = attn_config.model_config.use_dsa;
  o_proj_k_dim_ = head_num_per_atp_ * attn_config.model_config.mla_config.v_head_dim;
  use_q_lora_ = (attn_config.model_config.mla_config.q_lora_rank != 0);

  // attn_config.idx is the offset in kv_cache list.
  // e.g., master has normal layer 0-30 and nextn layer 61, the offset of layer_idx_61 is 31
  // e.g., master has normal layer 31-60, the offset of layer_idx_31 is 0
  attn_config.idx = layer_idx - creation_context.pipeline_config.lower_layer_idx;
  if (layer_idx >= static_cast<int>(model_creation_config.attn_config.model_config.num_layer)) {
    attn_config.idx = creation_context.pipeline_config.upper_layer_idx -
                      creation_context.pipeline_config.lower_layer_idx + layer_idx -
                      model_creation_config.attn_config.model_config.num_layer + 1;
  }

  // Initialize sparse MLA indexer if using sparse MLA
  if (use_dsa_) {
    sparse_mla_indexer_ = std::make_shared<SparseMlaIndexer>(layer_idx, creation_context, model_creation_config);
  }

  flash_mla_attention_layers_ = std::make_shared<FlashMlaAttention>(layer_idx, is_neox, creation_context, attn_config);
  paged_mla_attention_layers_ = std::make_shared<PagedMlaAttention>(layer_idx, is_neox, creation_context, attn_config);

  const std::string layer_prefix = fmt::format("model.layers.{}", layer_idx);
  kv_a_layernorms_ =
      std::make_shared<Layernorm>(layer_prefix + ".self_attn.kv_a_layernorm.weight",
                                  model_creation_config.layernorm_config.layernorm_eps, creation_context);
  q_a_layernorms_ = std::make_shared<Layernorm>(layer_prefix + ".self_attn.q_a_layernorm.weight",
                                                model_creation_config.layernorm_config.layernorm_eps, creation_context);

  const auto& linear_compute_backend = model_creation_config.attn_config.model_config.quant_config.backend;

  // TODO(huicongyao, jinxcwu): suppport INT4 model to keep use_fused_lora_a_ always true
  const std::string fused_lora_a_projs_weight_name = layer_prefix + ".self_attn.fused_lora_a_proj.weight";
  if (creation_context.base_weight->GetModelWeights(fused_lora_a_projs_weight_name).GetElementNumber() > 0) {
    use_fused_lora_a_ = true;
    attn_fused_lora_a_projs_ =
        std::make_shared<Linear>(fused_lora_a_projs_weight_name, creation_context, linear_compute_backend);
  } else {
    use_fused_lora_a_ = false;
    if (use_q_lora_) {
      attn_q_a_projs_ = std::make_shared<Linear>(layer_prefix + ".self_attn.q_a_proj.weight", creation_context,
                                                 linear_compute_backend);
    }
    attn_kv_a_lora_projs_ = std::make_shared<Linear>(layer_prefix + ".self_attn.kv_a_lora_proj.weight",
                                                     creation_context, linear_compute_backend);
    attn_kv_a_ropes_ = std::make_shared<Linear>(layer_prefix + ".self_attn.kv_a_rope_proj.weight", creation_context,
                                                linear_compute_backend);
  }

  attn_q_b_projs_ = std::make_shared<Linear>(layer_prefix + ".self_attn.q_b_nope_rope_proj.weight", creation_context,
                                             linear_compute_backend);

  split_ = std::make_shared<Split>(creation_context);

  attn_o_proj_ =
      std::make_shared<Linear>(layer_prefix + ".self_attn.o_proj.weight", creation_context, linear_compute_backend);
  attn_w_uk_t_bmm_ =
      std::make_shared<Bmm>(layer_prefix + ".self_attn.w_uk_t.weight", creation_context, linear_compute_backend);
  attn_w_uv_bmm_ =
      std::make_shared<Bmm>(layer_prefix + ".self_attn.w_uv.weight", creation_context, linear_compute_backend);

  if (o_proj_out_of_dp_) {
    mem_adjuster_ = std::make_shared<MemAdjuster>(creation_context);
  }

#ifdef ENABLE_CUDA
  set_torch_stream_layers_ = std::make_shared<SetTorchStreamLayer>();
  set_torch_stream_layers_->Init({}, creation_context.runtime_config, creation_context.context, creation_context.rank);
#endif
}

Status MultiHeadLatentAttention::CreateBuffers(BufferManager* buffer_mgr, const AttentionCreationConfig& attn_config,
                                               const RuntimeConfig& runtime_config, MlaBuffers& mla_buffers) {
  const DataType weight_type = attn_config.model_config.weight_data_type;
  const size_t max_token_num = runtime_config.max_step_token_num;
  const size_t head_num = attn_config.model_config.head_num;
  const size_t max_decode_tokens = runtime_config.max_batch_size * attn_config.max_decode_tokens_per_req;

  head_num_per_atp_ = head_num / runtime_config.parallel_basic_config.attn_tensor_parallel_size;
  qk_nope_head_dim_ = attn_config.model_config.mla_config.qk_nope_head_dim;
  qk_rope_head_dim_ = attn_config.model_config.mla_config.qk_rope_head_dim;
  kv_lora_rank_ = attn_config.model_config.mla_config.kv_lora_rank;
  q_lora_rank_ = attn_config.model_config.mla_config.q_lora_rank;

  const size_t kv_lora_or_q_nope_rope_buffer_size =
      max_token_num * std::max(head_num_per_atp_ * (qk_nope_head_dim_ + qk_rope_head_dim_), kv_lora_rank_);
  mla_buffers.kv_lora_or_q_nope_rope_buffer = buffer_mgr->CreateBufferTensor(
      "mla_buffers.kv_lora_or_q_nope_rope_buffer", {kv_lora_or_q_nope_rope_buffer_size}, weight_type);

  const size_t q_nope_buffer_size =
      attn_config.model_config.use_dsa ? 0 : max_decode_tokens * head_num_per_atp_ * kv_lora_rank_;
  mla_buffers.q_nope_buffer =
      buffer_mgr->CreateBufferTensor("mla_buffers.q_nope_buffer", {q_nope_buffer_size}, weight_type);

  const size_t q_rope_buffer_size =
      attn_config.model_config.use_dsa ? 0 : max_decode_tokens * head_num_per_atp_ * qk_rope_head_dim_;
  mla_buffers.q_rope_buffer =
      buffer_mgr->CreateBufferTensor("mla_buffers.q_rope_buffer", {q_rope_buffer_size}, weight_type);

  const size_t kv_buffer_size = max_token_num * kv_lora_rank_;
  mla_buffers.kv_buffer = buffer_mgr->CreateBufferTensor("mla_buffers.kv_buffer", {kv_buffer_size}, weight_type);

  const size_t k_rope_buffer_size = max_token_num * qk_rope_head_dim_;
  mla_buffers.k_rope_buffer =
      buffer_mgr->CreateBufferTensor("mla_buffers.k_rope_buffer", {k_rope_buffer_size}, weight_type);

  const size_t topk_indices_buffer_size =
      attn_config.model_config.use_dsa ? max_token_num * attn_config.model_config.dsa_config.index_topk : 0;
  mla_buffers.topk_indices_buffer =
      buffer_mgr->CreateBufferTensor("mla_buffers.topk_indices_buffer", {topk_indices_buffer_size}, TYPE_INT32);

  const size_t prefix_kv_buffer_size = (runtime_config.enable_prefix_caching && !attn_config.model_config.use_dsa)
                                           ? max_token_num * (kv_lora_rank_ + qk_rope_head_dim_)
                                           : 0;
  mla_buffers.shared_prefix_kv_buffer =
      buffer_mgr->CreateBufferTensor("mla_buffers.shared_prefix_kv_buffer", {prefix_kv_buffer_size}, weight_type);
  return Status();
}

Status MultiHeadLatentAttention::AcquireBuffers(ForwardingContext& forwarding_context) {
  // TODO(yancyliu): Get tensors from kv_lora_or_q_nope_rope_buffer, q_nope_buffer, q_rope_buffer,
  // kv_buffer, k_rope_buffer, topk_indices_buffer in mla_buffers_. Reset its shape from batch_size and token_num, and
  // then allocate tensor memory.
  return Status();
}

Status MultiHeadLatentAttention::ReleaseBuffers() {
  // TODO(yancyliu): Get tensor from kv_lora_or_q_nope_rope_buffer, q_nope_buffer, q_rope_buffer,
  // kv_buffer, k_rope_buffer, topk_indices_buffer in mla_buffers_. Then release tenosr memory.
  return Status();
}

Status MultiHeadLatentAttention::Forward(std::vector<Tensor>& hidden_buffer_tensors_0,
                                         std::vector<Tensor>& reduce_buffer_tensors,
                                         std::shared_ptr<TpCommunicator> tp_comm, bool is_multi_token_forward,
                                         ForwardingContext& forwarding_context) {
  const int rank = forwarding_context.GetCurrentRank();

  const size_t total_tokens = hidden_buffer_tensors_0[0].shape[0];
  const size_t hidden_units = hidden_buffer_tensors_0[0].shape[1];
  const size_t hidden_units_bytes = hidden_units * hidden_buffer_tensors_0[0].GetDTypeSize();

  // `dp_group_id` is responsible for the tokens in `[dp_token_offset, dp_token_offset + dp_context_tokens +
  // dp_decode_tokens)`
  // When disable attention dp, `dp_token_offset = 0, dp_context_tokens + dp_decode_tokens = total_tokens`
  const size_t dp_group_id = forwarding_context.GetModelInput()->attn_dp_group_id_;
  const int dp_token_offset = forwarding_context.GetModelInput()->attn_dp_group_offsets_[dp_group_id];
  const size_t dp_context_tokens = forwarding_context.GetModelInput()->dp_context_tokens;
  const size_t dp_decode_tokens = forwarding_context.GetModelInput()->dp_decode_tokens;
  const size_t dp_total_tokens = dp_context_tokens + dp_decode_tokens;
  // DeepSeek Sparse MLA applies weight absorption for all tokens,
  // while MLA only applies weight absorption for decode tokens
  const size_t dp_absorb_tokens = (use_dsa_ ? dp_total_tokens : dp_decode_tokens);
  KLLM_LOG_DEBUG << fmt::format(
      "rank: {}, dp_group_id: {}, dp_token_offset: {}, dp_context_tokens: {}, dp_decode_tokens: {}, dp_absorb_tokens: "
      "{}",
      rank, dp_group_id, dp_token_offset, dp_context_tokens, dp_decode_tokens, dp_absorb_tokens);

  PROFILE_EVENT_SCOPE(CommonAttention_seq_len_,
                      fmt::format("CommonAttention_seq_len_{}_hidden_units_{}", dp_total_tokens, hidden_units), rank);

#ifdef ENABLE_CUDA
  std::vector<Tensor> empty_tensors;
  set_torch_stream_layers_->Forward(empty_tensors, empty_tensors);
#endif

  // Only take tokens assigned to the current dp group
  const Tensor& dp_hidden_input =
      hidden_buffer_tensors_0[0].GetView({dp_total_tokens, hidden_units}, dp_token_offset * hidden_units);
  CREATE_BUFFER_SCOPE(hidden_buffer_tensors_1, forwarding_context.GetForwardingBuffers()->hidden_buffer_1);
  CREATE_BUFFER_SCOPE(kv_buffer_tensors, mla_buffers_.kv_buffer);
  CREATE_BUFFER_SCOPE(k_rope_buffer_tensors, mla_buffers_.k_rope_buffer);
  CREATE_BUFFER_SCOPE(q_nope_tensors, mla_buffers_.q_nope_buffer);
  CREATE_BUFFER_SCOPE(q_rope_tensors, mla_buffers_.q_rope_buffer);
  CREATE_BUFFER_SCOPE(q_nope_rope_buffer_tensors, mla_buffers_.kv_lora_or_q_nope_rope_buffer);
  CREATE_BUFFER_SCOPE(indices_tensors, mla_buffers_.topk_indices_buffer);
  auto& indices_tensor = indices_tensors[0];
  if (use_dsa_) {
    // Reshape `hidden_buffer_tensors_0`
    // to avoid memory checker error in the following `GetView`
    hidden_buffer_tensors_0[0].shape = {dp_total_tokens, head_num_per_atp_ * kv_lora_rank_};
    q_nope_tensors[0] = hidden_buffer_tensors_0[0].GetView({dp_absorb_tokens, head_num_per_atp_ * kv_lora_rank_});
    q_rope_tensors[0] = hidden_buffer_tensors_0[0].GetView({dp_absorb_tokens, head_num_per_atp_ * qk_rope_head_dim_},
                                                           q_nope_tensors[0].GetElementNumber());
  }

  if (dp_total_tokens > 0) {
    std::vector<Tensor>& kv_lora_buffer_tensors = q_nope_rope_buffer_tensors;
    std::vector<Tensor>& q_buffer_tensors = reduce_buffer_tensors;
    if (use_fused_lora_a_) {
      PROFILE_EVENT_SCOPE(attn_fused_lora_a_projs, "attn_fused_lora_a_proj", rank);
      // weight_shape = (q_lora_rank + kv_lora_rank + qk_rope_head_dim, hidden_units)
      STATUS_CHECK_RETURN(attn_fused_lora_a_projs_->Forward(dp_hidden_input, hidden_buffer_tensors_1));

      // split to q_buffer_tensors, kv_lora_buffer_tensors, k_rope_buffer_tensors
      q_buffer_tensors[0].shape = {dp_total_tokens, q_lora_rank_};
      kv_lora_buffer_tensors[0].shape = {dp_total_tokens, kv_lora_rank_};
      k_rope_buffer_tensors[0].shape = {dp_total_tokens, qk_rope_head_dim_};
      std::vector<Tensor> split_output_tensors = {q_buffer_tensors[0], kv_lora_buffer_tensors[0],
                                                  k_rope_buffer_tensors[0]};
      split_->Forward(hidden_buffer_tensors_1[0], split_output_tensors);
    } else {
      {  // kv_a_lora proj MatMul
        PROFILE_EVENT_SCOPE(attn_kv_a_lora_proj, "attn_kv_a_lora_proj", rank);
        STATUS_CHECK_RETURN(attn_kv_a_lora_projs_->Forward(
            dp_hidden_input, kv_lora_buffer_tensors));  // weight_shape = (kv_lora_rank, hidden_units)
      }
      {  // kv_a_rope_lora proj MatMul
        PROFILE_EVENT_SCOPE(kv_a_rope_proj, "kv_a_rope_proj", rank);
        STATUS_CHECK_RETURN(attn_kv_a_ropes_->Forward(
            dp_hidden_input, k_rope_buffer_tensors));  // weight_shape = (qk_rope_head_dim, hidden_units)
      }
    }
    {
      PROFILE_EVENT_SCOPE(kv_a_layernorm, "kv_a_layernorm", rank);
      kv_a_layernorms_->Forward(kv_lora_buffer_tensors, kv_buffer_tensors);
    }

    Tensor q_b_input = dp_hidden_input;
    // 降维度，q_lora_rank存在
    if (use_q_lora_) {
      if (!use_fused_lora_a_) {
        // q_a proj MatMul
        PROFILE_EVENT_SCOPE(q_a_proj, "q_a_proj", rank);
        // weight_shape = (q_lora_rank, hidden_units)
        STATUS_CHECK_RETURN(attn_q_a_projs_->Forward(dp_hidden_input, q_buffer_tensors));
      }
      {
        PROFILE_EVENT_SCOPE(q_a_layernorm, "q_a_layernorm", rank);
        q_a_layernorms_->Forward(q_buffer_tensors, hidden_buffer_tensors_1);
      }
      q_b_input = hidden_buffer_tensors_1[0];
    }

    if (use_dsa_) {
      KLLM_CHECK_WITH_INFO(use_q_lora_, "DeepSeek sparse attention must use q lora");
      STATUS_CHECK_RETURN(sparse_mla_indexer_->Forward(
          dp_hidden_input, q_b_input, /*workspace*/ reduce_buffer_tensors[0], indices_tensor, forwarding_context));
    }

    PROFILE_EVENT_SCOPE(q_b_nope_rope_proj_weight, "q_b_nope_rope_proj", rank);
    STATUS_CHECK_RETURN(attn_q_b_projs_->Forward(q_b_input, q_nope_rope_buffer_tensors));
    if (dp_absorb_tokens > 0) {
      Tensor q_nope_rope = q_nope_rope_buffer_tensors[0].GetView(
          {dp_absorb_tokens * head_num_per_atp_, (qk_nope_head_dim_ + qk_rope_head_dim_)},
          (dp_total_tokens - dp_absorb_tokens) * head_num_per_atp_ * (qk_nope_head_dim_ + qk_rope_head_dim_));

      // Split q_nope_rope to q_nope and q_rope
      q_buffer_tensors[0].shape = {dp_absorb_tokens * head_num_per_atp_, qk_nope_head_dim_};
      q_rope_tensors[0].shape = {dp_absorb_tokens * head_num_per_atp_, qk_rope_head_dim_};
      std::vector<Tensor> split_output_tensors = {q_buffer_tensors[0], q_rope_tensors[0]};
      split_->Forward(q_nope_rope, split_output_tensors);

      PROFILE_EVENT_SCOPE(attn_w_uk_t_bmm, "attn_w_uk_t_bmm", rank);
      // Reshape for the following bmm
      q_buffer_tensors[0].shape = {dp_absorb_tokens, head_num_per_atp_, qk_nope_head_dim_};
      // 融合Wuk到Qnope, 最后一维从qk_nope_dim变为kv_lora_rank
      STATUS_CHECK_RETURN(attn_w_uk_t_bmm_->Forward(q_buffer_tensors, q_nope_tensors));

      // Correctly set the shape
      q_rope_tensors[0].shape = {dp_absorb_tokens, head_num_per_atp_ * qk_rope_head_dim_};
      q_nope_tensors[0].shape = {dp_absorb_tokens, head_num_per_atp_ * kv_lora_rank_};
    }
  }

  // TODO(robertyuan): swap with reduce_buffer_tensors needs optimize.
  // Swap required: AllReduce only accepts reduce_buffer_tensors as input
  if (forwarding_context.GetModelCommunicator() && o_proj_out_of_dp_) {
    std::swap(hidden_buffer_tensors_0, reduce_buffer_tensors);
  }

  const size_t attn_output_offset = o_proj_out_of_dp_ ? dp_token_offset * o_proj_k_dim_ : 0;

  if (dp_context_tokens > 0) {
    PROFILE_EVENT_SCOPE(FlashAttentionForward, "FlashAttentionForward", forwarding_context.GetCurrentRank());
    CREATE_BUFFER_SCOPE(prefix_kv_buffer_tensors, mla_buffers_.shared_prefix_kv_buffer);

    // Prepare tensors for context forward
    Tensor context_q_nope_rope =
        use_dsa_ ? Tensor{}
                 : q_nope_rope_buffer_tensors[0].GetView({dp_context_tokens, q_nope_rope_buffer_tensors[0].shape[1]});
    Tensor context_q_nope =
        use_dsa_ ? q_nope_tensors[0].GetView({dp_context_tokens, q_nope_tensors[0].shape[1]}) : Tensor{};
    Tensor context_q_rope =
        use_dsa_ ? q_rope_tensors[0].GetView({dp_context_tokens, q_rope_tensors[0].shape[1]}) : Tensor{};

    // Swap the usage of output_tensors and workspace_buffer for absorb tokens
    // to ensure the result of bmm can be placed in output_tensors
    auto& workspace_tensors = (use_dsa_ ? /*unused*/ hidden_buffer_tensors_0 : reduce_buffer_tensors);
    std::vector<Tensor> attn_output_tensors = {
        use_dsa_ ? reduce_buffer_tensors[0].GetView({dp_context_tokens, head_num_per_atp_ * kv_lora_rank_})
                 : hidden_buffer_tensors_0[0].GetView({dp_context_tokens, o_proj_k_dim_}, attn_output_offset)};

    STATUS_CHECK_RETURN(flash_mla_attention_layers_->Forward(
        forwarding_context.GetModelInput(), forwarding_context.GetAttentionForwardContext(),
        /*k_buffer*/ hidden_buffer_tensors_1, /*v_buffer*/ workspace_tensors, context_q_nope_rope, context_q_nope,
        context_q_rope, kv_buffer_tensors[0], k_rope_buffer_tensors[0], prefix_kv_buffer_tensors[0], indices_tensor,
        attn_output_tensors));
  }

  if (dp_decode_tokens > 0) {
    PROFILE_EVENT_SCOPE(PagedAttentionForward, "PagedAttentionForward", forwarding_context.GetCurrentRank());

    // Reshape `reduce_buffer_tensors`
    // to avoid memory checker error in the following `GetView`
    reduce_buffer_tensors[0].shape = {dp_decode_tokens, head_num_per_atp_ * kv_lora_rank_};
    // Swap the usage of output_tensors and workspace_buffer for absorb tokens
    // to ensure the result of bmm can be placed in output_tensors
    Tensor attn_output_tensor =
        reduce_buffer_tensors[0].GetView({dp_decode_tokens, head_num_per_atp_ * kv_lora_rank_},
                                         (dp_absorb_tokens - dp_decode_tokens) * head_num_per_atp_ * kv_lora_rank_);

    // Process each page_input sequentially
    // Page_inputs are ordered by token_num in descending order, align with the order of forward requests
    size_t skip_tokens = dp_context_tokens;
    for (const auto& page_input : forwarding_context.GetModelInput()->page_inputs) {
      const size_t current_tokens = page_input.total_dp_input_ids_len;

      // Offset tensors by `skip_tokens` or `skip_tokens - dp_context_tokens`
      Tensor current_q_nope_tensor = q_nope_tensors[0].GetView(
          {current_tokens, q_nope_tensors[0].shape[1]},
          (use_dsa_ ? skip_tokens : skip_tokens - dp_context_tokens) * q_nope_tensors[0].shape[1]);
      Tensor current_q_rope_tensor = q_rope_tensors[0].GetView(
          {current_tokens, q_rope_tensors[0].shape[1]},
          (use_dsa_ ? skip_tokens : skip_tokens - dp_context_tokens) * q_rope_tensors[0].shape[1]);
      Tensor current_kv_buffer_tensor = kv_buffer_tensors[0].GetView({current_tokens, kv_buffer_tensors[0].shape[1]},
                                                                     skip_tokens * kv_buffer_tensors[0].shape[1]);
      Tensor current_k_rope_buffer_tensor = k_rope_buffer_tensors[0].GetView(
          {current_tokens, k_rope_buffer_tensors[0].shape[1]}, skip_tokens * k_rope_buffer_tensors[0].shape[1]);
      Tensor current_indices_tensor = use_dsa_ ? indices_tensor.GetView({current_tokens, indices_tensor.shape[1]},
                                                                        skip_tokens * indices_tensor.shape[1])
                                               : Tensor{};
      std::vector<Tensor> current_output_tensors = {
          attn_output_tensor.GetView({current_tokens, attn_output_tensor.shape[1]},
                                     (skip_tokens - dp_context_tokens) * attn_output_tensor.shape[1])};

      STATUS_CHECK_RETURN(paged_mla_attention_layers_->Forward(
          forwarding_context.GetModelInput(), page_input, forwarding_context.GetAttentionForwardContext(),
          hidden_buffer_tensors_1, current_q_nope_tensor, current_q_rope_tensor, current_kv_buffer_tensor,
          current_k_rope_buffer_tensor, current_indices_tensor, current_output_tensors));

      skip_tokens += current_tokens;
    }
  }

  if (dp_absorb_tokens > 0) {
    reduce_buffer_tensors[0].shape = {dp_absorb_tokens, head_num_per_atp_, kv_lora_rank_};
    std::vector<Tensor> absorb_attn_output_tensors = {hidden_buffer_tensors_0[0].GetView(
        {dp_absorb_tokens, o_proj_k_dim_}, (dp_total_tokens - dp_absorb_tokens) * o_proj_k_dim_ + attn_output_offset)};
    STATUS_CHECK_RETURN(attn_w_uv_bmm_->Forward(reduce_buffer_tensors, absorb_attn_output_tensors));
  }

  if (o_proj_out_of_dp_) {
    const size_t o_proj_k_dim_per_tp = o_proj_k_dim_ / tensor_parallel_size_;
    PROFILE_EVENT_SCOPE(o_proj, fmt::format("o_proj_ouf_of_dp_m_{}_n_{}", total_tokens, o_proj_k_dim_per_tp), rank);
    hidden_buffer_tensors_0[0].shape = {total_tokens, o_proj_k_dim_};
    if (dp_token_offset > 0) {
      // `[0, dp_token_offset)`
      MemsetAsync(hidden_buffer_tensors_0[0].GetPtr<void>(), 0,
                  dp_token_offset * o_proj_k_dim_ * hidden_buffer_tensors_0[0].GetDTypeSize(),
                  forwarding_context.GetContext()->GetComputeStreams()[rank]);
    }
    if (dp_token_offset + dp_total_tokens < total_tokens) {
      // `[dp_token_offset + dp_total_tokens, total_tokens)`
      MemsetAsync(hidden_buffer_tensors_0[0].GetPtr<void>() +
                      (dp_token_offset + dp_total_tokens) * o_proj_k_dim_ * hidden_buffer_tensors_0[0].GetDTypeSize(),
                  0,
                  (total_tokens - dp_token_offset - dp_total_tokens) * o_proj_k_dim_ *
                      hidden_buffer_tensors_0[0].GetDTypeSize(),
                  forwarding_context.GetContext()->GetComputeStreams()[rank]);
    }
    tp_comm->AllReduce(hidden_buffer_tensors_0, hidden_buffer_tensors_1, is_multi_token_forward, forwarding_context);

    mem_adjuster_->ExtractSubMatrix(hidden_buffer_tensors_1[0], reduce_buffer_tensors[0],
                                    o_proj_k_dim_per_tp * dp_group_id, o_proj_k_dim_per_tp);

    attn_o_proj_->Forward(reduce_buffer_tensors, hidden_buffer_tensors_0);
  } else if (dp_total_tokens > 0) {
    PROFILE_EVENT_SCOPE(o_proj, fmt::format("o_proj_m_{}_n_{}", dp_total_tokens, o_proj_k_dim_), rank);
    if (forwarding_context.GetModelCommunicator()) {
      std::swap(hidden_buffer_tensors_1, reduce_buffer_tensors);
    }
    // `hidden_buffer_tensors_0` is able to hold this space,
    // reshape to avoid memory checker error in the following `GetView`
    hidden_buffer_tensors_0[0].shape = {total_tokens, o_proj_k_dim_};
    Tensor o_input = hidden_buffer_tensors_0[0].GetView({dp_total_tokens, o_proj_k_dim_});
    // `hidden_buffer_tensors_1` is able to hold this space,
    // reshape to avoid memory checker error in the following `GetView`
    hidden_buffer_tensors_1[0].shape = {total_tokens, hidden_units};
    // Only output tokens assigned to the current dp group
    std::vector<Tensor> o_outputs = {
        hidden_buffer_tensors_1[0].GetView({dp_total_tokens, hidden_units}, dp_token_offset * hidden_units)};
    attn_o_proj_->Forward({o_input}, o_outputs);
    if (forwarding_context.GetModelCommunicator()) {
      std::swap(hidden_buffer_tensors_1, reduce_buffer_tensors);
    }
  }

  // swap back
  if (forwarding_context.GetModelCommunicator() && o_proj_out_of_dp_) {
    std::swap(hidden_buffer_tensors_0, reduce_buffer_tensors);
  }
  std::swap(hidden_buffer_tensors_1, hidden_buffer_tensors_0);

  if (forwarding_context.GetModelCommunicator()) {
    if (!o_proj_out_of_dp_) {
      // The output is now in the `reduce_buffer_tensors[0]` for the following allreduce
      // We should set the output of tokens not assigned to the current dp group to zero
      if (dp_token_offset > 0) {
        // `[0, dp_token_offset)`
        MemsetAsync(reduce_buffer_tensors[0].GetPtr<void>(), 0, dp_token_offset * hidden_units_bytes,
                    forwarding_context.GetContext()->GetComputeStreams()[rank]);
      }
      if (dp_token_offset + dp_total_tokens < total_tokens) {
        // `[dp_token_offset + dp_total_tokens, total_tokens)`
        MemsetAsync(reduce_buffer_tensors[0].GetPtr<void>() + (dp_token_offset + dp_total_tokens) * hidden_units_bytes,
                    0, (total_tokens - dp_token_offset - dp_total_tokens) * hidden_units_bytes,
                    forwarding_context.GetContext()->GetComputeStreams()[rank]);
      }
    }
    // correctly set the output shape
    reduce_buffer_tensors[0].shape = {total_tokens, hidden_units};
  } else {
    // The output is now in the `hidden_buffer_tensors_0[0]`
    // We should correctly set the output shape
    hidden_buffer_tensors_0[0].shape = {total_tokens, hidden_units};
  }

#ifdef ENABLE_CUDA
  set_torch_stream_layers_->Clear();
#endif

  return Status();
}

}  // namespace ksana_llm
