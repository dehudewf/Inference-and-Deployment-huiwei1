# 简历项目经历（STAR法则）

## 项目一：大模型推理框架源码分析与性能评测

**Situation**: 大模型推理部署面临显存管理低效、调度利用率不足等瓶颈，业界有vLLM、KsanaLLM等多个开源框架但缺乏系统性技术对比，尤其是腾讯开源的KsanaLLM技术文档较少

**Task**: 搭建vLLM推理环境完成性能测试，深入阅读KsanaLLM核心模块源码，对比两个框架的调度策略和缓存设计差异

**Action**:
- 在云A100上搭建vLLM环境，完成Qwen2.5-7B模型离线推理与API serving全流程，测试不同batch_size(1~32)下的吞吐量及prefix caching加速效果
- 深入阅读KsanaLLM源码，分析continuous_batching.h中ProcessWaitingQueue/ProcessDecodingQueue的三队列(Waiting/Running/Swapped)调度实现，以及prefix_cache_manager.h中PrefixCachedBlock树状前缀缓存的hash匹配与异步swap in/out机制
- 对比KsanaLLM与vLLM设计差异：Prefill-Decode解耦调度、Split-fuse分块Prefill、多硬件适配(GPU+华为NPU)、PP multi-batch跨机通讯量降低98.3%

**Result**: 掌握PagedAttention、Continuous Batching、Prefix Cache核心原理，建立源码级技术理解，输出7篇覆盖从KV Cache原理到框架源码的技术笔记

**技术栈**: Python, C++, vLLM, KsanaLLM, PyTorch, CUDA

---

## 项目二：FlashAttention CUDA Kernel性能分析与优化实践

**Situation**: FlashAttention通过利用GPU内存层次结构重新设计Attention算法实现2-4倍加速，是理解"软硬件协同优化"的典型案例，但其Tiling分块与Online Softmax机制较为复杂

**Task**: 从算法原理到CUDA实现完整理解FlashAttention，基于开源项目编写benchmark工具量化不同优化版本的性能差异

**Action**:
- 推导Online Softmax分块计算的数学原理，理解通过指数缩放在分块场景下维护全局max/sum实现等价全局Softmax
- 分析GPU内存层次对Attention性能的影响：Tiling分块将中间结果保留在SRAM(19TB/s)避免写回HBM(2TB/s)，显存复杂度从O(N²)降至O(N)
- 基于flash-attention-opt项目编写统一benchmark工具，对比baseline与三版优化Kernel在不同序列长度(256~4096)下的性能和显存占用

**Result**: v3版Warp级优化+Bank Conflict消除相比基础Tiling版本性能提升8-14倍；FlashAttention相比标准Attention在长序列场景下显存节省超50%

**技术栈**: CUDA, C++, PyTorch, Nsight Compute, GPU体系结构
