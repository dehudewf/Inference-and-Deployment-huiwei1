# Inference-and-Deployment-huiwei

LLM æ¨ç†ä¼˜åŒ–å­¦ä¹ ä¸å®è·µèµ„æºåº“

## ğŸ“ é¡¹ç›®ç»“æ„

```
.
â”œâ”€â”€ flash-attention-opt/     # Flash Attention CUDA ç®—å­ä¼˜åŒ–å®ç°
â”œâ”€â”€ vllm/                    # vLLM é«˜ååé‡æ¨ç†å¼•æ“æºç 
â”œâ”€â”€ KsanaLLM/                # è…¾è®¯ KsanaLLM æ¨ç†æ¡†æ¶æºç 
â””â”€â”€ docs/                    # å­¦ä¹ ç¬”è®°ä¸é¢è¯•å‡†å¤‡
```

## ğŸš€ Flash Attention ä¼˜åŒ–å®ç°

æ‰‹å†™ Flash Attention CUDA Kernelï¼Œä»åŸºç¡€ç‰ˆæœ¬é€æ­¥ä¼˜åŒ–åˆ°é«˜æ€§èƒ½ç‰ˆæœ¬ã€‚

### æ€§èƒ½å¯¹æ¯” (NVIDIA RTX 4060)

| `[bs, nh, N, M, d]` | Baseline | Minimal | v1 | v2 | v3 |
|:---:|:---:|:---:|:---:|:---:|:---:|
| [32, 8, 256, 256, 256] | 46.79ms | 741.07ms | 104.73ms | 127.37ms | **12.42ms** |
| [32, 8, 256, 256, 1024] | 56.87ms | 9544.46ms | 618.70ms | 542.99ms | **48.84ms** |
| [32, 8, 1024, 1024, 256] | 92.70ms | 11343.90ms | 1524.32ms | 2026.06ms | **167.24ms** |
| [32, 8, 1024, 1024, 1024] | 232.27ms | 153121ms | 10134.30ms | 8636.17ms | **707.79ms** |

### ä¼˜åŒ–ç‚¹

- **v1**: åŸºç¡€ tiling å®ç°
- **v2**: Shared memory ä¼˜åŒ–
- **v3**: Warp-level ä¼˜åŒ– + Bank conflict æ¶ˆé™¤

è¯¦è§ [flash-attention-opt/README.md](flash-attention-opt/README.md)

## ğŸ“š æ¨ç†æ¡†æ¶æºç åˆ†æ

### vLLM
- PagedAttention å†…å­˜ç®¡ç†
- Continuous Batching è°ƒåº¦
- Tensor Parallelism å®ç°

### KsanaLLM (è…¾è®¯)
- Prefix Cache æ ‘çŠ¶ç»“æ„è®¾è®¡
- å¼‚æ­¥ Swap in/out æœºåˆ¶
- Prefill/Decode é˜Ÿåˆ—è°ƒåº¦ç­–ç•¥

æ ¸å¿ƒæºç è·¯å¾„ï¼š
```
KsanaLLM/src/ksana_llm/batch_scheduler/strategy/continuous_batching.h
KsanaLLM/src/ksana_llm/cache_manager/prefix_cache_manager.h
```

## ğŸ“– å­¦ä¹ èµ„æ–™

| æ–‡ä»¶ | å†…å®¹ |
|------|------|
| `ç®€å†é¡¹ç›®_æ¨ç†ä¼˜åŒ–æ–¹å‘.md` | é¡¹ç›®ç»å† STAR æ³•åˆ™æè¿° |
| `è…¾è®¯æ··å…ƒæ¨ç†ä¼˜åŒ–é¢è¯•å‡†å¤‡æŒ‡å—.md` | é¢è¯•çŸ¥è¯†ç‚¹æ¢³ç† |
| `AIç¡•å£«æ±‚èŒå…¥é—¨åŸ¹è®­æ‰‹å†Œ.md` | æ±‚èŒå‡†å¤‡æŒ‡å— |

## ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

- CUDA >= 11.0
- CMake >= 3.18
- Python >= 3.8

### ç¼–è¯‘ Flash Attention

```bash
cd flash-attention-opt
mkdir build && cd build
cmake ..
make -j$(nproc)
```

## ğŸ“Œ æ ¸å¿ƒçŸ¥è¯†ç‚¹

### æ¨ç†ä¼˜åŒ–æŠ€æœ¯æ ˆ
- **ç®—å­ä¼˜åŒ–**: Flash Attention, Fused Kernel, Quantization (INT8/FP8)
- **å†…å­˜ç®¡ç†**: PagedAttention, KV Cache ä¼˜åŒ–, Prefix Caching
- **è°ƒåº¦ç­–ç•¥**: Continuous Batching, Chunked Prefill
- **å¹¶è¡Œç­–ç•¥**: Tensor Parallelism, Pipeline Parallelism

### å¸¸è§é¢è¯•é—®é¢˜
1. Flash Attention ä¸ºä»€ä¹ˆèƒ½å‡å°‘æ˜¾å­˜ï¼Ÿï¼ˆtiling + recomputationï¼‰
2. PagedAttention å¦‚ä½•ç®¡ç† KV Cacheï¼Ÿï¼ˆè™šæ‹Ÿå†…å­˜æ€æƒ³ï¼‰
3. Continuous Batching vs Static Batching çš„åŒºåˆ«ï¼Ÿ
4. Prefill å’Œ Decode é˜¶æ®µçš„è®¡ç®—ç‰¹ç‚¹ï¼Ÿ

## ğŸ“ License

æœ¬ä»“åº“ä»…ä¾›å­¦ä¹ äº¤æµä½¿ç”¨ã€‚

- vLLM: Apache 2.0 License
- KsanaLLM: Apache 2.0 License
