# 🎯 腾讯混元AI实习 - 1周冲刺战术手册

> **目标岗位**: 大模型高性能推理框架调度及模型并行优化研究
> **工作内容**: 针对长思维链(CoT)和多模态场景，完善多机调度方案，优化落地成本与耗时
> **准备时间**: 1周
> **你的背景**: 算法基础扎实，推理优化方向0基础

---

## 📊 Part 1: 岗位JD深度拆解

### 1.1 JD关键词映射

| JD关键词                 | 技术映射                                     | 重要性     |
| ------------------------ | -------------------------------------------- | ---------- |
| **推理框架调度**   | Continuous Batching, Batch Scheduler         | ⭐⭐⭐⭐⭐ |
| **模型并行优化**   | Tensor Parallel (TP), Pipeline Parallel (PP) | ⭐⭐⭐⭐   |
| **长思维链(CoT)**  | 长序列KV Cache管理, Prefix Cache, Swap策略   | ⭐⭐⭐⭐   |
| **多模态场景**     | 图文混合推理, 不同模态调度                   | ⭐⭐⭐     |
| **多机调度**       | 跨机通信优化, PP multi-batch                 | ⭐⭐⭐⭐⭐ |
| **落地成本与耗时** | 吞吐量, 延迟, 显存优化                       | ⭐⭐⭐⭐   |

### 1.2 你需要展示的能力

```
┌─────────────────────────────────────────────────────────────────┐
│                    面试官想看到的能力                             │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  1. 理解推理优化的核心问题                                        │
│     └── "为什么LLM推理是访存密集型？如何优化？"                   │
│                                                                 │
│  2. 掌握主流技术方案                                             │
│     └── "PagedAttention、Continuous Batching、FlashAttention"  │
│                                                                 │
│  3. 了解腾讯自研项目（加分！）                                    │
│     └── "我研究过KsanaLLM的源码..."                             │
│                                                                 │
│  4. 具备学习和成长潜力                                           │
│     └── "虽然经验有限，但我的学习方法是..."                      │
│                                                                 │
└─────────────────────────────────────────────────────────────────┘
```

---

## 📚 Part 2: 知识点优先级排序

### 2.1 P0级：必须滚瓜烂熟（面试必问）

| 知识点              | 预计面试权重 | 学习时间 |
| ------------------- | ------------ | -------- |
| PagedAttention原理  | 25%          | 3h       |
| Continuous Batching | 25%          | 3h       |
| KV Cache加速原理    | 15%          | 2h       |
| KsanaLLM技术亮点    | 15%          | 3h       |

### 2.2 P1级：要理解原理（大概率会问）

| 知识点                 | 预计面试权重 | 学习时间 |
| ---------------------- | ------------ | -------- |
| FlashAttention为什么快 | 10%          | 2h       |
| TP/PP并行策略选择      | 5%           | 1.5h     |
| 长序列(CoT)挑战        | 5%           | 1h       |

### 2.3 P2级：了解概念即可（可能追问）

| 知识点               | 学习时间 |
| -------------------- | -------- |
| 多模态推理特点       | 30min    |
| Speculative Decoding | 30min    |
| 量化对推理影响       | 30min    |

---

## 📅 Part 3: 7天冲刺计划

### Day 1: 基础概念日

**目标**: 理解LLM推理的本质和KV Cache

```
□ 09:00-11:00  理解Transformer自回归推理过程
               - 为什么每次只生成一个token？
               - Prefill阶段 vs Decode阶段的区别
           
□ 11:00-12:00  理解KV Cache加速原理
               - 为什么K、V可以缓存？
               - 画出有/无KV Cache的计算流程图
           
□ 14:00-17:00  精读vLLM官方博客
               - https://blog.vllm.ai/2023/06/20/vllm.html
               - 重点理解内存浪费问题
           
□ 19:00-21:00  整理笔记，用自己的话写一遍
```

**今日输出**:

- [ ] 能画出KV Cache工作原理图
- [ ] 能解释Prefill和Decode的区别
- [ ] 能说出传统KV Cache的内存浪费问题

---

### Day 2: PagedAttention专题日

**目标**: 深入理解PagedAttention，这是面试重中之重

```
□ 09:00-12:00  PagedAttention原理学习
               - 类比操作系统虚拟内存分页
               - Block的概念和作用
               - 物理块 vs 逻辑块
           
□ 14:00-16:00  理解KV Cache共享机制
               - Beam Search场景
               - 并行采样场景
               - Copy-on-Write机制
           
□ 16:00-18:00  阅读vLLM论文（选读核心章节）
               - arxiv:2309.06180
               - 重点看Figure 3-5
           
□ 19:00-21:00  准备面试回答，反复练习讲解
```

**今日输出**:

- [ ] 能画出PagedAttention的Block映射图
- [ ] 能解释为什么能减少内存碎片
- [ ] 能说出性能提升数据（vs HuggingFace 14-24x）

---

### Day 3: Continuous Batching + FlashAttention日

**目标**: 理解调度策略和底层优化

```
□ 09:00-12:00  Continuous Batching深入理解
               - Static Batching的问题
               - Token级调度 vs 请求级调度
               - Waiting/Running/Swapped三个队列
           
□ 14:00-17:00  FlashAttention原理
               - 传统Attention的O(N²)内存问题
               - Tiling分块计算思想
               - Online Softmax算法（理解即可）
               - 关键数字：SRAM 19TB/s vs HBM 2TB/s
           
□ 19:00-21:00  整理笔记，画流程图
```

**今日输出**:

- [ ] 能画出Continuous Batching调度流程
- [ ] 能解释FlashAttention为什么快（一句话版本）
- [ ] 记住带宽数字：SRAM 19TB/s，HBM 2TB/s

---

### Day 4: KsanaLLM源码日（腾讯项目，重点加分！）

**目标**: 深入了解腾讯自研项目，面试主动提及

```
□ 09:00-10:30  阅读KsanaLLM README
               - 项目定位和技术亮点
               - 多硬件支持（GPU + NPU）
               - 性能数据
           
□ 10:30-13:00  阅读 continuous_batching.h
               路径: KsanaLLM/src/ksana_llm/batch_scheduler/strategy/
               - ProcessWaitingQueue 函数
               - ProcessDecodingQueue 函数
               - 理解调度策略
           
□ 14:00-17:00  阅读 prefix_cache_manager.h
               路径: KsanaLLM/src/ksana_llm/cache_manager/
               - PrefixCachedBlock 树状结构
               - Hash匹配机制
               - Swap in/out 异步处理
           
□ 19:00-21:00  整理KsanaLLM技术笔记
               - 记住2-3个具体的类名/函数名
               - 对比vLLM的设计差异
```

**今日输出**:

- [ ] 能说出KsanaLLM的3个技术亮点
- [ ] 能提到具体的源码文件/函数名
- [ ] 能说出多机调度优化（跨机通讯降低98.3%）

---

### Day 5: 实践体验日

**目标**: 跑通vLLM，建立实际操作经验

```
□ 09:00-12:00  环境准备和vLLM安装
               pip install vllm
               # 如果没有GPU，用CPU模式或云GPU
           
□ 14:00-17:00  跑通vLLM示例
               - 离线推理示例
               - API Server示例（如果有GPU）
               - 记录启动命令和关键参数
           
□ 17:00-18:00  观察和记录
               - 截图保存
               - 记录实际运行的命令
               - 观察输出格式
           
□ 19:00-21:00  整理实践笔记
```

**vLLM快速体验代码**:

```python
# 最简离线推理示例
from vllm import LLM, SamplingParams

# 如果没有大GPU，用小模型测试
llm = LLM(model="facebook/opt-125m")  # 小模型，CPU也能跑
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=100)

prompts = ["Hello, my name is"]
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Generated: {output.outputs[0].text}")
```

**今日输出**:

- [ ] 成功运行vLLM示例
- [ ] 记录启动命令和参数
- [ ] 截图保存作为面试素材

---

### Day 6: 面试问答准备日

**目标**: 准备所有可能的面试问题

```
□ 09:00-12:00  准备P0级问题答案（见Part 4）
               - 反复朗读，确保流畅
               - 录音听自己的回答
           
□ 14:00-16:00  准备P1级问题答案
               - TP/PP选择
               - 长序列挑战
               - FlashAttention细节
           
□ 16:00-18:00  准备项目讲解（STAR法则）
               - 1分钟版本
               - 3分钟详细版本
           
□ 19:00-21:00  准备"为什么选这个方向"等软性问题
```

**今日输出**:

- [ ] 所有P0问题能流畅回答
- [ ] 项目讲解控制在3分钟内
- [ ] 准备好诚实应对不会问题的话术

---

### Day 7: 模拟面试 + 最终检查日

**目标**: 查漏补缺，调整状态

```
□ 09:00-11:00  自我模拟面试
               - 对着镜子/录像讲项目
               - 计时控制
               - 发现表达问题
           
□ 11:00-12:00  回顾所有笔记
               - 快速过一遍知识点
               - 强化记忆薄弱点
           
□ 14:00-16:00  最终Checklist检查
               - 确认所有"必须能回答"的问题
               - 确认关键数字记忆
           
□ 16:00-18:00  准备面试材料
               - 简历打印/电子版
               - 笔记本（记录面试官问题）
           
□ 晚上        早睡，保证精神状态
```

**今日输出**:

- [ ] 完成自我模拟面试
- [ ] 所有Checklist打勾
- [ ] 心态调整到位

---

## 💬 Part 4: 必背面试问答

### 4.1 P0级问题（必问，必须秒答）

#### Q1: KV Cache为什么能加速推理？

```
标准答案:

自回归模型生成时，每次只生成一个token。关键观察是：
- 之前token的K、V值不会改变，只有Q在变
- 没有KV Cache：每生成一个token，都要重新计算所有token的K、V
- 有KV Cache：只计算新token的K、V，然后和缓存的拼接

计算量从 O(n²) 降到 O(n)，这就是加速的本质。

[画图辅助]: 
第1次: 计算 K1,V1
第2次: 复用 K1,V1，只算 K2,V2
第3次: 复用 K1,V1,K2,V2，只算 K3,V3
...
```

#### Q2: PagedAttention原理？

```
标准答案:

PagedAttention借鉴了操作系统虚拟内存的分页技术。

传统问题:
- KV Cache需要连续内存预分配
- 不同请求长度不同，导致60-80%内存浪费

PagedAttention解决方案:
- 将KV Cache分成固定大小的Block（如16个token一块）
- 非连续存储，按需分配
- 通过Block Table维护逻辑块到物理块的映射

额外优势:
- 支持KV Cache共享（beam search、并行采样）
- 使用Copy-on-Write机制

性能: vs HuggingFace 14-24x吞吐提升，vs TGI 2.2-3.5x提升
```

#### Q3: Continuous Batching是什么？

```
标准答案:

传统Static Batching的问题:
- 一个batch内所有请求必须同时完成
- 短请求等长请求，GPU利用率低

Continuous Batching是token级调度:
- 每生成一个token，就可以调整batch组成
- 已完成的请求立即移出，新请求可以插入
- 维护三个队列：Waiting（等待）、Running（运行中）、Swapped（换出）

好处:
- GPU利用率大幅提升
- 吞吐量提高2-3倍
- 支持动态优先级调度
```

#### Q4: FlashAttention为什么快？

```
标准答案:

传统Attention问题:
- 需要存储 N×N 的中间矩阵(Q×K^T)到HBM
- 内存复杂度 O(N²)
- 频繁读写HBM（慢）

FlashAttention解决方案:
1. Tiling分块计算: 把Q、K、V分成小块处理
2. 中间结果留在SRAM: 不写回HBM
3. Online Softmax: 分块计算时维护全局max和sum

关键数字:
- A100 SRAM带宽: 19TB/s
- A100 HBM带宽: 2TB/s（差10倍！）

效果: 加速2-4倍，内存降到O(N)，减少5-20倍
```

#### Q5: KsanaLLM有什么技术亮点？（腾讯项目！）

```
标准答案:

KsanaLLM是腾讯PCG开源的推理引擎，我研究过它的源码。主要亮点:

1. 多硬件支持:
   - 同时支持Nvidia GPU和华为NPU
   - 适配国产化需求

2. 高效调度:
   - 改进的Continuous Batching实现
   - 异步调度策略
   - 源码在 batch_scheduler/strategy/continuous_batching.h

3. Prefix Cache设计:
   - 树状结构存储 (PrefixCachedBlock)
   - Hash匹配实现前缀复用
   - 支持异步swap in/out
   - 源码在 cache_manager/prefix_cache_manager.h

4. 多机调度优化（v0.6.0）:
   - PP multi-batch
   - 跨机通讯量降低98.3%
   - 支持TCP机间通讯

性能: 比vLLM/TensorRT-LLM推理单价低20%+
```

### 4.2 P1级问题（大概率会问）

#### Q6: TP和PP什么时候用哪个？

```
Tensor Parallel (TP):
- 适合高带宽互联（NVLink 600GB/s）
- 通常在单机多卡内使用
- 切分方式：列切分MLP，行切分Attention

Pipeline Parallel (PP):
- 适合低带宽场景（跨机网络）
- 按layer切分模型
- 需要micro-batch减少bubble

混合策略:
- 机内用TP（利用NVLink高带宽）
- 机间用PP（减少跨机通信）

KsanaLLM的优化: PP multi-batch减少了98.3%的跨机通讯量
```

#### Q7: 长思维链(CoT)场景有什么挑战？

```
CoT意味着输出序列特别长（几千到几万tokens），挑战:

1. KV Cache显存压力:
   - 序列越长，KV Cache越大
   - 可能需要swap到CPU内存

2. Decode阶段占比大:
   - 生成tokens多，decode成为主要耗时
   - 需要高效的decode优化

3. 优化方案:
   - FlashAttention：内存O(N²)→O(N)
   - Prefix Cache：多轮对话复用前缀KV
   - Speculative Decoding：小模型draft加速
   - KV Cache压缩/量化
```

#### Q8: 大模型推理是计算密集还是访存密集？

```
答案: Decode阶段是访存密集型！

分析:
- Prefill阶段: 计算密集（大量矩阵乘法）
- Decode阶段: 访存密集（每次只算1个token，但要读取全部KV Cache）

计算强度 = FLOPs / Memory Access

Decode时:
- 计算量小（batch_size × hidden_dim）
- 访存量大（读取所有KV Cache）
- 计算强度低 → 访存瓶颈

优化方向:
- 增大batch_size提高计算强度
- 减少内存访问（FlashAttention）
- KV Cache量化减少访存量
```

### 4.3 项目讲解STAR版本

#### 1分钟版本

```
"在准备推理优化方向实习时，我系统学习了主流推理框架。

我重点研究了腾讯开源的KsanaLLM项目的源码。通过阅读continuous_batching.h
和prefix_cache_manager.h，我理解了它的调度策略和缓存设计。

KsanaLLM用树状结构实现Prefix Cache，通过hash匹配实现KV复用，这对多轮对话
场景很重要。它的多机调度优化也很impressive，PP multi-batch把跨机通讯量
降低了98.3%。

我也跑通了vLLM的示例，理解了PagedAttention和Continuous Batching的实际效果。
虽然我还没有深入修改代码的经验，但我对这些核心技术的原理有了扎实的理解。"
```

#### 3分钟详细版本

```
Situation:
"在准备推理优化方向的实习时，我发现业界有多个开源推理框架，但缺乏系统性的
对比分析。特别是腾讯开源的KsanaLLM，我想通过源码研究深入理解其设计。"

Task:
"我给自己设定了几个目标：
1. 跑通vLLM，理解实际serving流程
2. 阅读KsanaLLM源码，理解核心模块设计
3. 对比两个框架的技术差异"

Action:
"首先，我搭建了vLLM环境，跑通了离线推理示例。通过实际操作，我理解了
PagedAttention的Block分配过程和Continuous Batching的调度逻辑。

然后，我深入阅读了KsanaLLM的源码。在continuous_batching.h中，我看到了
ProcessWaitingQueue和ProcessDecodingQueue的实现，理解了它如何管理
waiting、running、swapped三个队列。

在prefix_cache_manager.h中，我学习了PrefixCachedBlock的树状结构设计，
理解了如何通过token序列的hash匹配来查找可复用的KV Cache块。

我还特别关注了KsanaLLM的多机调度优化，它在v0.6.0实现的PP multi-batch
把跨机通讯量降低了98.3%，这对长思维链场景的多机推理很重要。"

Result:
"通过这个学习过程，我：
1. 掌握了PagedAttention、Continuous Batching、Prefix Cache的核心原理
2. 理解了KsanaLLM相比vLLM的设计差异：异步调度、多硬件支持、通信优化
3. 建立了源码阅读能力，能够从代码层面理解技术实现

虽然我没有修改框架代码的经验，但我相信这些理论基础能帮助我快速上手实际工作。"
```

---

## 🛡️ Part 5: 风险应对策略

### 5.1 风险矩阵

| 风险                   | 概率 | 影响 | 应对策略                              |
| ---------------------- | ---- | ---- | ------------------------------------- |
| 被深挖技术细节答不上来 | 高   | 中   | 诚实承认，展示学习思路                |
| 没有实际项目经验       | 高   | 中   | 强调"源码阅读+原理理解"               |
| 被问CUDA/C++基础       | 中   | 中   | 说"正在学习中"，展示计划              |
| 被问没准备的知识点     | 中   | 低   | "这个我还没深入研究，但我的理解是..." |
| 紧张导致表达不清       | 中   | 高   | 提前模拟，准备关键句                  |

### 5.2 诚实应对话术

**当被问到不会的深入细节时:**

```
"这部分我主要是阅读源码理解原理，没有实际修改过代码。
但我理解它的设计思路是... [说你知道的部分]"
```

**当被问到有没有实践经验时:**

```
"我时间有限，主要做了两件事：
一是跑通vLLM示例，理解serving流程；
二是阅读KsanaLLM源码，特别是continuous_batching和prefix_cache_manager模块。
我理解原理但还没有深入优化的经验，这也是我希望通过实习来积累的。"
```

**当被问到完全不懂的概念时:**

```
"这个概念我还没有深入学习，但根据名字我猜测可能是... 
您能简单介绍一下吗？我很想了解。"
```

**当被追问"你做的最有挑战的事是什么"时:**

```
"最有挑战的是理解Online Softmax算法——如何在分块计算时正确维护全局的max和sum。
我花了不少时间画图推导，最终理解了它通过指数的scale来实现数值稳定的分块计算。"
```

---

## 🎯 Part 6: 面试当天Checklist

### 面试前一天

- [ ] 早睡（保证7-8小时睡眠）
- [ ] 准备好简历（电子版+打印版）
- [ ] 检查面试设备（如果是视频面试）
- [ ] 复习关键数字和话术

### 面试当天

- [ ] 提前15分钟到达/上线
- [ ] 准备纸笔（随时画图解释）
- [ ] 手机静音
- [ ] 深呼吸，调整心态

### 必须能画的图

- [ ] KV Cache工作原理图
- [ ] PagedAttention Block映射图
- [ ] Continuous Batching调度流程图
- [ ] FlashAttention Tiling示意图（简化版）

### 必须记住的数字

- [ ] PagedAttention vs HuggingFace: 14-24x吞吐提升
- [ ] A100 SRAM带宽: 19TB/s
- [ ] A100 HBM带宽: 2TB/s
- [ ] KsanaLLM跨机通讯优化: 降低98.3%
- [ ] FlashAttention加速: 2-4x，内存减少5-20x

### 必须能说的话术

- [ ] "我研究过腾讯的KsanaLLM源码..."
- [ ] "continuous_batching.h中的ProcessWaitingQueue..."
- [ ] "prefix_cache_manager用树状结构实现..."
- [ ] "我理解原理但还没有深入修改代码的经验..."

---

## 💡 Part 7: 加分话术

### 开场自我介绍（1分钟）

```
"您好，我是XXX，目前本科四年级，即将去香港城市大学读硕士。

我对大模型推理优化方向很感兴趣。最近系统学习了vLLM和KsanaLLM两个框架，
特别深入阅读了KsanaLLM的源码，对Continuous Batching调度和Prefix Cache
设计有了理解。

我了解到腾讯混元在长思维链和多模态场景有大量优化需求，KsanaLLM的多机调度
优化也给我留下了深刻印象。我希望能加入团队，将我的算法背景和对系统优化的
兴趣结合起来，做出有落地价值的贡献。"
```

### 回答"为什么选这个方向"

```
"我选择推理优化方向，有三个原因：

第一，我认为这是大模型商业化的关键瓶颈。训练是一次性的，但推理是持续的成本。
优化推理效率直接影响产品体验和商业可行性。

第二，这个方向需要算法和系统的结合能力。我有算法背景，同时对底层系统很感兴趣。
FlashAttention就是一个很好的例子——它利用GPU硬件特性重新设计算法。

第三，我研究过腾讯的KsanaLLM项目，对其多硬件支持和异步调度设计印象深刻。
特别是针对长思维链场景的优化，我觉得这是一个很有意义的技术方向。"
```

### 回答"你有什么问题想问我们"

```
准备2-3个问题:

1. "KsanaLLM在长思维链场景下，KV Cache的swap策略是怎么设计的？
    有没有考虑预测性的预加载？"

2. "团队目前在多模态推理调度上有什么挑战？
    图像encoder和文本decoder的调度是分开还是统一的？"

3. "对于实习生，您建议我重点学习哪些技术栈或工具？"
```

---

## 📖 Part 8: 学习资源汇总

### 必看资源（按优先级）

| 优先级     | 资源               | 链接                                      | 预计时间   |
| ---------- | ------------------ | ----------------------------------------- | ---------- |
| ⭐⭐⭐⭐⭐ | vLLM官方博客       | https://blog.vllm.ai/2023/06/20/vllm.html | 2h         |
| ⭐⭐⭐⭐⭐ | KsanaLLM README    | 本地已clone                               | 1h         |
| ⭐⭐⭐⭐   | FlashAttention博客 | https://hub.baai.ac.cn/view/29867         | 2h         |
| ⭐⭐⭐⭐   | vLLM论文           | arxiv:2309.06180                          | 2h（选读） |

### 源码阅读路径

```
KsanaLLM/
├── src/ksana_llm/
│   ├── batch_scheduler/
│   │   └── strategy/
│   │       └── continuous_batching.h  ← 重点阅读
│   └── cache_manager/
│       └── prefix_cache_manager.h     ← 重点阅读
```

### 关键函数/类名（面试可提）

```cpp
// KsanaLLM调度相关
ProcessWaitingQueue()
ProcessDecodingQueue()
ContinuousBatchingStrategy

// KsanaLLM缓存相关
PrefixCachedBlock
PrefixCacheManager
GetCachedBlocks()
```
